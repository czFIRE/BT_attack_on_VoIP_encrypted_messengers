{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication of attack on VoIP end-to-end encrypted messengers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to explore various models on `Whatsapp` dataset. Bellow we will find loading and preprocessing that we have come up with in the analysis section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "sns.set()  # make plots nicer\n",
    "\n",
    "np.random.seed(42)  # set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_parser_with_prev_next(path):\n",
    "    file = open(path, 'r')\n",
    "    lines = file.readlines()\n",
    "    \n",
    "    file_name = [path.split('/')[-1]]\n",
    "    sentence = \"\"\n",
    "    file_data = []\n",
    "    \n",
    "    has_value = False\n",
    "    previous = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # if there are only 2 informations on line and second is h#, then ignore\n",
    "        # strip line, split primarly on ; secondary on ,\n",
    "        if (line.startswith('#')):\n",
    "            if (not sentence):\n",
    "                sentence = line[len('# Sentence: \"'): len(line) - 1]\n",
    "            continue\n",
    "        \n",
    "        line = line.split(';')\n",
    "        \n",
    "        if (len(line) == 1):\n",
    "            #lines containing only their packet size and nothing else, they should be added\n",
    "            #TODO\n",
    "            line += [\"\"]\n",
    "            line += [\"\"]\n",
    "            #continue\n",
    "        \n",
    "        if (len(line) == 2):\n",
    "            #this tries to remove most of the silence at the start of the recording\n",
    "            #potentionally harmfull as we shouldn't clean test data this way (we will be reading labels)\n",
    "            #if (line[1] == 'h#'):\n",
    "            #    continue\n",
    "            line += [\"\"]\n",
    "        \n",
    "        line[1] = tuple(line[1].split(','))\n",
    "        line[2] = tuple(list(map(lambda a: a.strip('\"'), line[2].split(','))))\n",
    "        \n",
    "        if (has_value):\n",
    "            file_data[-1][4] = line[0]\n",
    "           \n",
    "        # file_type and sentence contain duplicate informations, but are kept for readability\n",
    "        line = file_name + [file_name[0][0:9]] + [sentence] + [previous] + [0] + line\n",
    "        #adding previous as feature\n",
    "        previous = line[5]\n",
    "        file_data += [line]\n",
    "        \n",
    "        #adding next frame as feature\n",
    "        has_value = True\n",
    "        \n",
    "        \n",
    "\n",
    "    return pd.DataFrame(file_data, columns=['file', 'speaker', 'sentence', 'previous_packet', 'next_packet','packet_size', 'phonemes', 'words'])\n",
    "\n",
    "def load_files_with_prev_next(directory):\n",
    "    filelist = os.listdir(directory)\n",
    "    #read them into pandas\n",
    "    df_list = [file_parser_with_prev_next(directory+file) for file in filelist]\n",
    "    #concatenate them together\n",
    "    return pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_types(data_frame):\n",
    "    data_frame['packet_size'] = pd.to_numeric(data_frame['packet_size'])\n",
    "    data_frame['previous_packet'] = pd.to_numeric(data_frame['previous_packet'])\n",
    "    data_frame['next_packet'] = pd.to_numeric(data_frame['next_packet'])\n",
    "\n",
    "    data_frame['file'] = data_frame['file'].astype('category')\n",
    "    data_frame['sentence'] = data_frame['sentence'].astype('category')\n",
    "    data_frame['speaker'] = data_frame['speaker'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>speaker</th>\n",
       "      <th>sentence</th>\n",
       "      <th>previous_packet</th>\n",
       "      <th>next_packet</th>\n",
       "      <th>packet_size</th>\n",
       "      <th>phonemes</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DR1-FAKS0-SA1.CSV</td>\n",
       "      <td>DR1-FAKS0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>0</td>\n",
       "      <td>342</td>\n",
       "      <td>249</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DR1-FAKS0-SA1.CSV</td>\n",
       "      <td>DR1-FAKS0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>249</td>\n",
       "      <td>335</td>\n",
       "      <td>342</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DR1-FAKS0-SA1.CSV</td>\n",
       "      <td>DR1-FAKS0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>342</td>\n",
       "      <td>303</td>\n",
       "      <td>335</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR1-FAKS0-SA1.CSV</td>\n",
       "      <td>DR1-FAKS0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>335</td>\n",
       "      <td>364</td>\n",
       "      <td>303</td>\n",
       "      <td>(h#, sh)</td>\n",
       "      <td>(she,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DR1-FAKS0-SA1.CSV</td>\n",
       "      <td>DR1-FAKS0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>303</td>\n",
       "      <td>418</td>\n",
       "      <td>364</td>\n",
       "      <td>(sh, iy, hv)</td>\n",
       "      <td>(she, had)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31584</th>\n",
       "      <td>DR8-MSLB0-SX383.CSV</td>\n",
       "      <td>DR8-MSLB0</td>\n",
       "      <td>The carpet cleaners shampooed our oriental rug.</td>\n",
       "      <td>338</td>\n",
       "      <td>370</td>\n",
       "      <td>303</td>\n",
       "      <td>(r, ao, r)</td>\n",
       "      <td>(our, oriental)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31585</th>\n",
       "      <td>DR8-MSLB0-SX383.CSV</td>\n",
       "      <td>DR8-MSLB0</td>\n",
       "      <td>The carpet cleaners shampooed our oriental rug.</td>\n",
       "      <td>303</td>\n",
       "      <td>314</td>\n",
       "      <td>370</td>\n",
       "      <td>(r, iy, eh)</td>\n",
       "      <td>(oriental,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31586</th>\n",
       "      <td>DR8-MSLB0-SX383.CSV</td>\n",
       "      <td>DR8-MSLB0</td>\n",
       "      <td>The carpet cleaners shampooed our oriental rug.</td>\n",
       "      <td>370</td>\n",
       "      <td>303</td>\n",
       "      <td>314</td>\n",
       "      <td>(eh, n, tcl, t)</td>\n",
       "      <td>(oriental,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31587</th>\n",
       "      <td>DR8-MSLB0-SX383.CSV</td>\n",
       "      <td>DR8-MSLB0</td>\n",
       "      <td>The carpet cleaners shampooed our oriental rug.</td>\n",
       "      <td>314</td>\n",
       "      <td>295</td>\n",
       "      <td>303</td>\n",
       "      <td>(t, el, r, ah)</td>\n",
       "      <td>(oriental, rug)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31588</th>\n",
       "      <td>DR8-MSLB0-SX383.CSV</td>\n",
       "      <td>DR8-MSLB0</td>\n",
       "      <td>The carpet cleaners shampooed our oriental rug.</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "      <td>295</td>\n",
       "      <td>(ah, gcl)</td>\n",
       "      <td>(rug,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31589 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      file    speaker  \\\n",
       "0        DR1-FAKS0-SA1.CSV  DR1-FAKS0   \n",
       "1        DR1-FAKS0-SA1.CSV  DR1-FAKS0   \n",
       "2        DR1-FAKS0-SA1.CSV  DR1-FAKS0   \n",
       "3        DR1-FAKS0-SA1.CSV  DR1-FAKS0   \n",
       "4        DR1-FAKS0-SA1.CSV  DR1-FAKS0   \n",
       "...                    ...        ...   \n",
       "31584  DR8-MSLB0-SX383.CSV  DR8-MSLB0   \n",
       "31585  DR8-MSLB0-SX383.CSV  DR8-MSLB0   \n",
       "31586  DR8-MSLB0-SX383.CSV  DR8-MSLB0   \n",
       "31587  DR8-MSLB0-SX383.CSV  DR8-MSLB0   \n",
       "31588  DR8-MSLB0-SX383.CSV  DR8-MSLB0   \n",
       "\n",
       "                                                sentence  previous_packet  \\\n",
       "0      She had your dark suit in greasy wash water al...                0   \n",
       "1      She had your dark suit in greasy wash water al...              249   \n",
       "2      She had your dark suit in greasy wash water al...              342   \n",
       "3      She had your dark suit in greasy wash water al...              335   \n",
       "4      She had your dark suit in greasy wash water al...              303   \n",
       "...                                                  ...              ...   \n",
       "31584    The carpet cleaners shampooed our oriental rug.              338   \n",
       "31585    The carpet cleaners shampooed our oriental rug.              303   \n",
       "31586    The carpet cleaners shampooed our oriental rug.              370   \n",
       "31587    The carpet cleaners shampooed our oriental rug.              314   \n",
       "31588    The carpet cleaners shampooed our oriental rug.              303   \n",
       "\n",
       "       next_packet  packet_size         phonemes            words  \n",
       "0              342          249            (h#,)              (,)  \n",
       "1              335          342            (h#,)              (,)  \n",
       "2              303          335            (h#,)              (,)  \n",
       "3              364          303         (h#, sh)           (she,)  \n",
       "4              418          364     (sh, iy, hv)       (she, had)  \n",
       "...            ...          ...              ...              ...  \n",
       "31584          370          303       (r, ao, r)  (our, oriental)  \n",
       "31585          314          370      (r, iy, eh)      (oriental,)  \n",
       "31586          303          314  (eh, n, tcl, t)      (oriental,)  \n",
       "31587          295          303   (t, el, r, ah)  (oriental, rug)  \n",
       "31588            0          295        (ah, gcl)           (rug,)  \n",
       "\n",
       "[31589 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whatsapp_data_train = load_files_with_prev_next(\"./../data/whatsapp_train_data/\")\n",
    "whatsapp_data_test = load_files_with_prev_next(\"./../data/whatsapp_test_data/\")\n",
    "convert_types(whatsapp_data_train)\n",
    "convert_types(whatsapp_data_test)\n",
    "whatsapp_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>speaker</th>\n",
       "      <th>sentence</th>\n",
       "      <th>previous_packet</th>\n",
       "      <th>next_packet</th>\n",
       "      <th>packet_size</th>\n",
       "      <th>prev_curr</th>\n",
       "      <th>next_curr</th>\n",
       "      <th>packet_surrounding</th>\n",
       "      <th>phonemes</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DR1-FCJF0-SA1.CSV</td>\n",
       "      <td>DR1-FCJF0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>227</td>\n",
       "      <td>(0, 227)</td>\n",
       "      <td>(380, 227)</td>\n",
       "      <td>(0, 227, 380)</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DR1-FCJF0-SA1.CSV</td>\n",
       "      <td>DR1-FCJF0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>227</td>\n",
       "      <td>407</td>\n",
       "      <td>380</td>\n",
       "      <td>(227, 380)</td>\n",
       "      <td>(407, 380)</td>\n",
       "      <td>(227, 380, 407)</td>\n",
       "      <td>(h#, sh, ix)</td>\n",
       "      <td>(she,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DR1-FCJF0-SA1.CSV</td>\n",
       "      <td>DR1-FCJF0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>380</td>\n",
       "      <td>350</td>\n",
       "      <td>407</td>\n",
       "      <td>(380, 407)</td>\n",
       "      <td>(350, 407)</td>\n",
       "      <td>(380, 407, 350)</td>\n",
       "      <td>(ix, hv, eh)</td>\n",
       "      <td>(she, had)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR1-FCJF0-SA1.CSV</td>\n",
       "      <td>DR1-FCJF0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>407</td>\n",
       "      <td>281</td>\n",
       "      <td>350</td>\n",
       "      <td>(407, 350)</td>\n",
       "      <td>(281, 350)</td>\n",
       "      <td>(407, 350, 281)</td>\n",
       "      <td>(eh, dcl, jh)</td>\n",
       "      <td>(had, your)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DR1-FCJF0-SA1.CSV</td>\n",
       "      <td>DR1-FCJF0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>350</td>\n",
       "      <td>327</td>\n",
       "      <td>281</td>\n",
       "      <td>(350, 281)</td>\n",
       "      <td>(327, 281)</td>\n",
       "      <td>(350, 281, 327)</td>\n",
       "      <td>(jh, ih, dcl, d, ah)</td>\n",
       "      <td>(had, your, dark)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86492</th>\n",
       "      <td>DR8-MTCS0-SX82.CSV</td>\n",
       "      <td>DR8-MTCS0</td>\n",
       "      <td>Good service should be rewarded by big tips.</td>\n",
       "      <td>286</td>\n",
       "      <td>253</td>\n",
       "      <td>268</td>\n",
       "      <td>(286, 268)</td>\n",
       "      <td>(253, 268)</td>\n",
       "      <td>(286, 268, 253)</td>\n",
       "      <td>(ay, bcl, b, ih)</td>\n",
       "      <td>(by, big)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86493</th>\n",
       "      <td>DR8-MTCS0-SX82.CSV</td>\n",
       "      <td>DR8-MTCS0</td>\n",
       "      <td>Good service should be rewarded by big tips.</td>\n",
       "      <td>268</td>\n",
       "      <td>315</td>\n",
       "      <td>253</td>\n",
       "      <td>(268, 253)</td>\n",
       "      <td>(315, 253)</td>\n",
       "      <td>(268, 253, 315)</td>\n",
       "      <td>(ih, gcl)</td>\n",
       "      <td>(big,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86494</th>\n",
       "      <td>DR8-MTCS0-SX82.CSV</td>\n",
       "      <td>DR8-MTCS0</td>\n",
       "      <td>Good service should be rewarded by big tips.</td>\n",
       "      <td>253</td>\n",
       "      <td>279</td>\n",
       "      <td>315</td>\n",
       "      <td>(253, 315)</td>\n",
       "      <td>(279, 315)</td>\n",
       "      <td>(253, 315, 279)</td>\n",
       "      <td>(gcl, t, ih)</td>\n",
       "      <td>(big, tips)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86495</th>\n",
       "      <td>DR8-MTCS0-SX82.CSV</td>\n",
       "      <td>DR8-MTCS0</td>\n",
       "      <td>Good service should be rewarded by big tips.</td>\n",
       "      <td>315</td>\n",
       "      <td>392</td>\n",
       "      <td>279</td>\n",
       "      <td>(315, 279)</td>\n",
       "      <td>(392, 279)</td>\n",
       "      <td>(315, 279, 392)</td>\n",
       "      <td>(ih, pcl, p)</td>\n",
       "      <td>(tips,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86496</th>\n",
       "      <td>DR8-MTCS0-SX82.CSV</td>\n",
       "      <td>DR8-MTCS0</td>\n",
       "      <td>Good service should be rewarded by big tips.</td>\n",
       "      <td>279</td>\n",
       "      <td>0</td>\n",
       "      <td>392</td>\n",
       "      <td>(279, 392)</td>\n",
       "      <td>(0, 392)</td>\n",
       "      <td>(279, 392, 0)</td>\n",
       "      <td>(p, s, h#)</td>\n",
       "      <td>(tips,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86497 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     file    speaker  \\\n",
       "0       DR1-FCJF0-SA1.CSV  DR1-FCJF0   \n",
       "1       DR1-FCJF0-SA1.CSV  DR1-FCJF0   \n",
       "2       DR1-FCJF0-SA1.CSV  DR1-FCJF0   \n",
       "3       DR1-FCJF0-SA1.CSV  DR1-FCJF0   \n",
       "4       DR1-FCJF0-SA1.CSV  DR1-FCJF0   \n",
       "...                   ...        ...   \n",
       "86492  DR8-MTCS0-SX82.CSV  DR8-MTCS0   \n",
       "86493  DR8-MTCS0-SX82.CSV  DR8-MTCS0   \n",
       "86494  DR8-MTCS0-SX82.CSV  DR8-MTCS0   \n",
       "86495  DR8-MTCS0-SX82.CSV  DR8-MTCS0   \n",
       "86496  DR8-MTCS0-SX82.CSV  DR8-MTCS0   \n",
       "\n",
       "                                                sentence  previous_packet  \\\n",
       "0      She had your dark suit in greasy wash water al...                0   \n",
       "1      She had your dark suit in greasy wash water al...              227   \n",
       "2      She had your dark suit in greasy wash water al...              380   \n",
       "3      She had your dark suit in greasy wash water al...              407   \n",
       "4      She had your dark suit in greasy wash water al...              350   \n",
       "...                                                  ...              ...   \n",
       "86492       Good service should be rewarded by big tips.              286   \n",
       "86493       Good service should be rewarded by big tips.              268   \n",
       "86494       Good service should be rewarded by big tips.              253   \n",
       "86495       Good service should be rewarded by big tips.              315   \n",
       "86496       Good service should be rewarded by big tips.              279   \n",
       "\n",
       "       next_packet  packet_size   prev_curr   next_curr packet_surrounding  \\\n",
       "0              380          227    (0, 227)  (380, 227)      (0, 227, 380)   \n",
       "1              407          380  (227, 380)  (407, 380)    (227, 380, 407)   \n",
       "2              350          407  (380, 407)  (350, 407)    (380, 407, 350)   \n",
       "3              281          350  (407, 350)  (281, 350)    (407, 350, 281)   \n",
       "4              327          281  (350, 281)  (327, 281)    (350, 281, 327)   \n",
       "...            ...          ...         ...         ...                ...   \n",
       "86492          253          268  (286, 268)  (253, 268)    (286, 268, 253)   \n",
       "86493          315          253  (268, 253)  (315, 253)    (268, 253, 315)   \n",
       "86494          279          315  (253, 315)  (279, 315)    (253, 315, 279)   \n",
       "86495          392          279  (315, 279)  (392, 279)    (315, 279, 392)   \n",
       "86496            0          392  (279, 392)    (0, 392)      (279, 392, 0)   \n",
       "\n",
       "                   phonemes              words  \n",
       "0                     (h#,)                (,)  \n",
       "1              (h#, sh, ix)             (she,)  \n",
       "2              (ix, hv, eh)         (she, had)  \n",
       "3             (eh, dcl, jh)        (had, your)  \n",
       "4      (jh, ih, dcl, d, ah)  (had, your, dark)  \n",
       "...                     ...                ...  \n",
       "86492      (ay, bcl, b, ih)          (by, big)  \n",
       "86493             (ih, gcl)             (big,)  \n",
       "86494          (gcl, t, ih)        (big, tips)  \n",
       "86495          (ih, pcl, p)            (tips,)  \n",
       "86496            (p, s, h#)            (tips,)  \n",
       "\n",
       "[86497 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_surrounding(data_frame):\n",
    "    data_frame['prev_curr'] = list(zip(data_frame.previous_packet, data_frame.packet_size))\n",
    "    data_frame['next_curr'] = list(zip(data_frame.next_packet, data_frame.packet_size))\n",
    "    data_frame['packet_surrounding'] = list(zip(data_frame.previous_packet, data_frame.packet_size, data_frame.next_packet))\n",
    "    \n",
    "    #data_frame['prev_curr'] = data_frame['prev_curr'].astype('category')\n",
    "    #data_frame['next_curr'] = data_frame['next_curr'].astype('category')\n",
    "    #data_frame['packet_surrounding'] = data_frame['packet_surrounding'].astype('category')\n",
    "\n",
    "add_surrounding(whatsapp_data_train)\n",
    "add_surrounding(whatsapp_data_test)\n",
    "\n",
    "whatsapp_data_train = whatsapp_data_train[['file', 'speaker', 'sentence', 'previous_packet', 'next_packet','packet_size', 'prev_curr', 'next_curr', 'packet_surrounding', 'phonemes', 'words']]\n",
    "whatsapp_data_test = whatsapp_data_test[['file', 'speaker', 'sentence', 'previous_packet', 'next_packet','packet_size', 'prev_curr', 'next_curr', 'packet_surrounding', 'phonemes', 'words']]\n",
    "whatsapp_data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add removal of labels for the test_dataset\n",
    "def get_labels(df, label=[\"words\"], feature=[\"previous_packet\", \"packet_size\", \"next_packet\"]):\n",
    "    labels = df.loc[:, label]\n",
    "    features = df.loc[:, feature]\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels(train_labels, test_labels, label=[\"words\"]):\n",
    "    train_labels = train_labels.astype('category')\n",
    "    test_labels = test_labels.astype('category')\n",
    "    \n",
    "    total_labels = train_labels.append(test_labels)\n",
    "    \n",
    "    lab_enc = LabelEncoder()\n",
    "    lab_enc.fit(total_labels[label])\n",
    "\n",
    "    train_labels = lab_enc.transform(train_labels[label])\n",
    "    test_labels = lab_enc.transform(test_labels[label])\n",
    "    \n",
    "    return train_labels, test_labels, lab_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16168\n",
      "6739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21317"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train)\n",
    "test_set, test_labels = get_labels(whatsapp_data_test)\n",
    "\n",
    "train_labels = train_labels.astype('category')\n",
    "test_labels = test_labels.astype('category')\n",
    "\n",
    "total_labels = train_labels.append(test_labels)\n",
    "print(len(pd.unique(train_labels.words)))\n",
    "print(len(pd.unique(test_labels.words)))\n",
    "total_unique_words = len(pd.unique(total_labels.words))\n",
    "total_unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that we have a really big problem => there are 5149 new words that we have never seen. As we saw in our analysis we can't really generalise on never seen words before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27369\n",
      "12655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33990"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=['phonemes'])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=['phonemes'])\n",
    "\n",
    "train_labels = train_labels.astype('category')\n",
    "test_labels = test_labels.astype('category')\n",
    "\n",
    "total_labels = train_labels.append(test_labels)\n",
    "print(len(pd.unique(train_labels.phonemes)))\n",
    "print(len(pd.unique(test_labels.phonemes)))\n",
    "total_unique_phonemes = len(pd.unique(total_labels.phonemes))\n",
    "total_unique_phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With phonemes the situation is a bit different, as there are more phonemes and we haven't seen only half of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First model that we will be trying is tree classifier. In the analysis we have noticed, that there is almost a 1:1 correspondence of trigram of phoneme sizes and words (eg. that for every trigram of phoneme sizes there is different word). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train)\n",
    "test_set, test_labels = get_labels(whatsapp_data_test)\n",
    "\n",
    "train_labels = train_labels.astype('category')\n",
    "test_labels = test_labels.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16168\n",
      "6739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21317"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_labels = train_labels.append(test_labels)\n",
    "print(len(pd.unique(train_labels.words)))\n",
    "print(len(pd.unique(test_labels.words)))\n",
    "len(pd.unique(total_labels.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0, 15548, 15578, ...,  2531, 18676, 18676])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_enc = LabelEncoder()\n",
    "lab_enc.fit(total_labels.words)\n",
    "\n",
    "train_labels = lab_enc.transform(train_labels.words)\n",
    "test_labels = lab_enc.transform(test_labels.words)\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"clf\",\n",
    "            DecisionTreeClassifier(criterion=\"entropy\", max_depth=None, splitter=\"best\",\n",
    "                                   min_samples_split=2, random_state=42),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# Words: criterion=\"entropy\", max_depth=None, splitter=\"best\", min_samples_split=2, random_state=42 => 0.97, 0.02\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.2631\n",
      "Test accuracy : 0.0380\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "tree_clf_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "print(f\"Train accuracy: {tree_clf_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {tree_clf_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12858, 13445, 15553, ..., 12837, 14816, 24059])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"phonemes\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=[\"phonemes\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels, label=[\"phonemes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.2454\n",
      "Test accuracy : 0.0293\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "tree_clf_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "print(f\"Train accuracy: {tree_clf_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {tree_clf_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have been able to run these classificators and the best results I was able to get were around 3%, which isn't that good considering KNN was able to get twice that much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look a different kind of classifier => k nearest neighbours. This classifier shouldn't need that much RAM and that much of a computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\swdevelopment\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"words\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=[\"words\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"clf\",\n",
    "            KNeighborsClassifier(20, weights='distance', n_jobs=4)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# train accuracy 0.9723\n",
    "#5   => 0.0313\n",
    "#10  => 0.0377\n",
    "#20  => 0.0450\n",
    "#32  => 0.0497\n",
    "#64  => 0.0567\n",
    "#128 => 0.0625\n",
    "#256 => 0.0668\n",
    "\n",
    "# uniform gives better test results but doesn't seem to be able to \"answer correctly\" on the train test\n",
    "# 64 => 0.0927, 0.0685"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9723\n",
      "Test accuracy : 0.0438\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "knn_clf_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "print(f\"Train accuracy: {knn_clf_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {knn_clf_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the search space of 64 nearest neighbours we get only 5.67% success rate on our test data (which is around 1971 words). I have listed other parameters and their resulting percentages in the comments in the code cell. Also worth noting is that \"StandardScaler\" only worsens our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This whole section is just made as sanity check that we actually get expected results (that is we only guess the words we've already seen and none from which we haven't seen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably remove -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known words:\t 16405\n",
      "Unknown words:\t 15184\n"
     ]
    }
   ],
   "source": [
    "data_test_copy = whatsapp_data_test.copy()\n",
    "\n",
    "column_select = list(map(lambda x: x in list(whatsapp_data_train.words.drop_duplicates()), list(data_test_copy.words)))\n",
    "\n",
    "print(\"Known words:\\t\", column_select.count(True))\n",
    "print(\"Unknown words:\\t\", column_select.count(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_copy = data_test_copy[column_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\swdevelopment\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"words\"])\n",
    "test_set, test_labels = get_labels(data_test_copy, label=[\"words\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.0516\n",
      "Test accuracy : 0.0518\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "# knn_clf_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "# 256, distance => 0.9723, 0.1286 on only \n",
    "\n",
    "print(f\"Train accuracy: {knn_clf_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {knn_clf_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we get that the succes rate on known is around double the ammount on all words (this can be seen from the output of a cell 2 cells above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\swdevelopment\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"words\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test[list(map(lambda x: not x, column_select))], label=[\"words\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9723\n",
      "Test accuracy : 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "#knn_clf_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "print(f\"Train accuracy: {knn_clf_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {knn_clf_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test was only made as \"sanity check\" as it is indeed highly probable that our model wouldn't be able to properly guess on never seen examples of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To here remove -------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try our luck with phonemes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\swdevelopment\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"phonemes\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=[\"phonemes\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels, label=[\"phonemes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"clf\",\n",
    "            KNeighborsClassifier(16, weights='distance', n_jobs=4)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# 5 => 0.0256\n",
    "# 6 => 0.0269\n",
    "# 10 => 0.0299\n",
    "# 20 => 0.0343\n",
    "# 32 => 0.0368\n",
    "# 64 => 0.0404\n",
    "# 128 => 0.0441\n",
    "# 256 => 0.0465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9723\n",
      "Test accuracy : 0.0324\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "knn_clf_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "print(f\"Train accuracy: {knn_clf_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {knn_clf_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that phonemes didn't help us that much and that the results are far worse from those gotten by exploring words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xkadlec6/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"words\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=[\"words\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"clf\",\n",
    "            RandomForestClassifier(max_depth=12, random_state=42, criterion = 'entropy', n_jobs = -1, min_samples_split = 2)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# (max_depth=12, random_state=42, criterion = 'entropy', n_jobs = -1, min_samples_split = 2) => 0.6181, 0.0651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting!\n",
      "Finished!\n",
      "Train accuracy: 0.6181\n",
      "Test accuracy : 0.0651\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "rfc_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "print(f\"Train accuracy: {rfc_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {rfc_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that this indeed has better accuracy than normal tree / KNN, but takes way more system resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xkadlec6/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"phonemes\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=[\"phonemes\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels, label=[\"phonemes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"clf\",\n",
    "            RandomForestClassifier(max_depth=8, random_state=42, criterion = 'entropy', n_jobs = -1, min_samples_split = 2)\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting!\n",
      "Finished!\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 17.6 GiB for an array with shape (86497, 27369) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-02d269ded994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train accuracy: {rfc_pipeline.score(train_set, train_labels):.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test accuracy : {rfc_pipeline.score(test_set, test_labels):.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0mscore_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mscore_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \"\"\"\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;31m# avoid storing the output of every estimator by summing them here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         all_proba = [np.zeros((X.shape[0], j), dtype=np.float64)\n\u001b[0;32m--> 681\u001b[0;31m                      for j in np.atleast_1d(self.n_classes_)]\n\u001b[0m\u001b[1;32m    682\u001b[0m         \u001b[0mlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         Parallel(n_jobs=n_jobs, verbose=self.verbose,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;31m# avoid storing the output of every estimator by summing them here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         all_proba = [np.zeros((X.shape[0], j), dtype=np.float64)\n\u001b[0;32m--> 681\u001b[0;31m                      for j in np.atleast_1d(self.n_classes_)]\n\u001b[0m\u001b[1;32m    682\u001b[0m         \u001b[0mlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         Parallel(n_jobs=n_jobs, verbose=self.verbose,\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 17.6 GiB for an array with shape (86497, 27369) and data type float64"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "rfc_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "print(f\"Train accuracy: {rfc_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {rfc_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xkadlec6/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"words\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=[\"words\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"clf\",\n",
    "            AdaBoostClassifier(random_state=1, n_estimators = 60, learning_rate=0.9)\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0505, 0.0471\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "#abc_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "#print(f\"Train accuracy: {abc_pipeline.score(train_set, train_labels):.4f}\")\n",
    "#print(f\"Test accuracy : {abc_pipeline.score(test_set, test_labels):.4f}\")\n",
    "\n",
    "print(\"0.0505, 0.0471\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier ended in absolute failure as it wasn't able to get even acceptable results on the train data. And it even took 8 hours to learn (this is because it can only use 1 thread), so this classifier is pretty much worthless to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xkadlec6/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"words\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=[\"words\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86497, 3)\n",
      "(86497,)\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86497, 21317)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels, num_classes=total_unique_words)\n",
    "test_labels = to_categorical(test_labels, num_classes=total_unique_words)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 21317)             5478469   \n",
      "=================================================================\n",
      "Total params: 5,611,845\n",
      "Trainable params: 5,611,845\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=512, activation='relu', input_dim=3*1))  # first hidden layer\n",
    "model.add(Dense(units=256, activation='relu'))  # second hidden layer\n",
    "# model.add(Dense(units=128, activation='relu'))  # third hidden layer\n",
    "model.add(Dense(units=total_unique_words, activation='softmax'))  # output layer\n",
    "# model.add(Dense(units=128))  # output layer\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "338/338 [==============================] - 18s 52ms/step - loss: 8.7460 - accuracy: 0.0448\n",
      "Epoch 2/64\n",
      "338/338 [==============================] - 17s 50ms/step - loss: 7.8892 - accuracy: 0.0536\n",
      "Epoch 3/64\n",
      "338/338 [==============================] - 17s 49ms/step - loss: 7.7529 - accuracy: 0.05410s - loss: 7.7524 - accura\n",
      "Epoch 4/64\n",
      "338/338 [==============================] - 17s 50ms/step - loss: 7.6726 - accuracy: 0.0554\n",
      "Epoch 5/64\n",
      "338/338 [==============================] - 17s 49ms/step - loss: 7.5973 - accuracy: 0.0552\n",
      "Epoch 6/64\n",
      "338/338 [==============================] - 16s 47ms/step - loss: 7.5415 - accuracy: 0.0561\n",
      "Epoch 7/64\n",
      "338/338 [==============================] - 15s 46ms/step - loss: 7.4971 - accuracy: 0.0571\n",
      "Epoch 8/64\n",
      "338/338 [==============================] - 15s 46ms/step - loss: 7.4539 - accuracy: 0.0568\n",
      "Epoch 9/64\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 7.3924 - accuracy: 0.0593\n",
      "Epoch 10/64\n",
      "338/338 [==============================] - 15s 45ms/step - loss: 7.3484 - accuracy: 0.0595\n",
      "Epoch 11/64\n",
      "338/338 [==============================] - 15s 45ms/step - loss: 7.3250 - accuracy: 0.0595\n",
      "Epoch 12/64\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 7.3048 - accuracy: 0.0584\n",
      "Epoch 13/64\n",
      "338/338 [==============================] - 15s 44ms/step - loss: 7.2846 - accuracy: 0.0591\n",
      "Epoch 14/64\n",
      "338/338 [==============================] - 15s 46ms/step - loss: 7.2424 - accuracy: 0.0599\n",
      "Epoch 15/64\n",
      "338/338 [==============================] - 15s 45ms/step - loss: 7.2156 - accuracy: 0.0615\n",
      "Epoch 16/64\n",
      "338/338 [==============================] - 15s 45ms/step - loss: 7.2107 - accuracy: 0.0610\n",
      "Epoch 17/64\n",
      "338/338 [==============================] - 15s 46ms/step - loss: 7.1977 - accuracy: 0.0608\n",
      "Epoch 18/64\n",
      "338/338 [==============================] - 15s 45ms/step - loss: 7.1900 - accuracy: 0.0614\n",
      "Epoch 19/64\n",
      "338/338 [==============================] - 15s 44ms/step - loss: 7.1668 - accuracy: 0.0605\n",
      "Epoch 20/64\n",
      "338/338 [==============================] - 15s 44ms/step - loss: 7.1508 - accuracy: 0.0613\n",
      "Epoch 21/64\n",
      "338/338 [==============================] - 15s 45ms/step - loss: 7.1452 - accuracy: 0.0618\n",
      "Epoch 22/64\n",
      "338/338 [==============================] - 15s 43ms/step - loss: 7.1046 - accuracy: 0.0624\n",
      "Epoch 23/64\n",
      "338/338 [==============================] - 15s 45ms/step - loss: 7.1108 - accuracy: 0.0611\n",
      "Epoch 24/64\n",
      "338/338 [==============================] - 15s 45ms/step - loss: 7.1046 - accuracy: 0.0620\n",
      "Epoch 25/64\n",
      "338/338 [==============================] - 15s 44ms/step - loss: 7.0863 - accuracy: 0.0640\n",
      "Epoch 26/64\n",
      "338/338 [==============================] - 15s 44ms/step - loss: 7.0747 - accuracy: 0.0628\n",
      "Epoch 27/64\n",
      "338/338 [==============================] - 15s 43ms/step - loss: 7.0877 - accuracy: 0.0610\n",
      "Epoch 28/64\n",
      "338/338 [==============================] - 15s 43ms/step - loss: 7.0496 - accuracy: 0.0645\n",
      "Epoch 29/64\n",
      "338/338 [==============================] - 15s 46ms/step - loss: 7.0676 - accuracy: 0.0624\n",
      "Epoch 30/64\n",
      "338/338 [==============================] - 16s 48ms/step - loss: 7.0293 - accuracy: 0.0641\n",
      "Epoch 31/64\n",
      "338/338 [==============================] - 15s 46ms/step - loss: 7.0297 - accuracy: 0.0634\n",
      "Epoch 32/64\n",
      "338/338 [==============================] - 15s 45ms/step - loss: 7.0339 - accuracy: 0.0650\n",
      "Epoch 33/64\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 7.0330 - accuracy: 0.0635\n",
      "Epoch 34/64\n",
      "338/338 [==============================] - 16s 47ms/step - loss: 6.9983 - accuracy: 0.0657\n",
      "Epoch 35/64\n",
      "338/338 [==============================] - 16s 47ms/step - loss: 7.0159 - accuracy: 0.0643\n",
      "Epoch 36/64\n",
      "338/338 [==============================] - 15s 46ms/step - loss: 7.0033 - accuracy: 0.0632\n",
      "Epoch 37/64\n",
      "338/338 [==============================] - 15s 45ms/step - loss: 6.9833 - accuracy: 0.0652\n",
      "Epoch 38/64\n",
      "338/338 [==============================] - 15s 45ms/step - loss: 6.9633 - accuracy: 0.0651\n",
      "Epoch 39/64\n",
      "338/338 [==============================] - 15s 45ms/step - loss: 6.9845 - accuracy: 0.0642\n",
      "Epoch 40/64\n",
      "338/338 [==============================] - 15s 44ms/step - loss: 6.9727 - accuracy: 0.0642\n",
      "Epoch 41/64\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 6.9576 - accuracy: 0.0664\n",
      "Epoch 42/64\n",
      "338/338 [==============================] - 15s 43ms/step - loss: 6.9625 - accuracy: 0.0647\n",
      "Epoch 43/64\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 6.9528 - accuracy: 0.0644\n",
      "Epoch 44/64\n",
      "338/338 [==============================] - 15s 43ms/step - loss: 6.9715 - accuracy: 0.0635\n",
      "Epoch 45/64\n",
      "338/338 [==============================] - 14s 43ms/step - loss: 6.9562 - accuracy: 0.0653\n",
      "Epoch 46/64\n",
      "338/338 [==============================] - 14s 43ms/step - loss: 6.9469 - accuracy: 0.0651\n",
      "Epoch 47/64\n",
      "338/338 [==============================] - 15s 44ms/step - loss: 6.9422 - accuracy: 0.0659\n",
      "Epoch 48/64\n",
      "338/338 [==============================] - 15s 45ms/step - loss: 6.9305 - accuracy: 0.0668\n",
      "Epoch 49/64\n",
      "338/338 [==============================] - 15s 43ms/step - loss: 6.9407 - accuracy: 0.0664\n",
      "Epoch 50/64\n",
      "338/338 [==============================] - 15s 44ms/step - loss: 6.9396 - accuracy: 0.0652\n",
      "Epoch 51/64\n",
      "338/338 [==============================] - 15s 44ms/step - loss: 6.9362 - accuracy: 0.0657\n",
      "Epoch 52/64\n",
      "338/338 [==============================] - 15s 45ms/step - loss: 6.9277 - accuracy: 0.0672\n",
      "Epoch 53/64\n",
      "338/338 [==============================] - 15s 44ms/step - loss: 6.9231 - accuracy: 0.0672\n",
      "Epoch 54/64\n",
      "338/338 [==============================] - 14s 43ms/step - loss: 6.9278 - accuracy: 0.0655\n",
      "Epoch 55/64\n",
      "338/338 [==============================] - 15s 44ms/step - loss: 6.9202 - accuracy: 0.0640\n",
      "Epoch 56/64\n",
      "338/338 [==============================] - 15s 45ms/step - loss: 6.9286 - accuracy: 0.0656\n",
      "Epoch 57/64\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 6.9028 - accuracy: 0.0664\n",
      "Epoch 58/64\n",
      "338/338 [==============================] - 15s 44ms/step - loss: 6.8930 - accuracy: 0.0671\n",
      "Epoch 59/64\n",
      "338/338 [==============================] - 15s 44ms/step - loss: 6.9115 - accuracy: 0.0677\n",
      "Epoch 60/64\n",
      "338/338 [==============================] - 16s 47ms/step - loss: 6.9123 - accuracy: 0.0669\n",
      "Epoch 61/64\n",
      "338/338 [==============================] - 14s 43ms/step - loss: 6.9012 - accuracy: 0.0655\n",
      "Epoch 62/64\n",
      "338/338 [==============================] - 15s 46ms/step - loss: 6.8974 - accuracy: 0.0656\n",
      "Epoch 63/64\n",
      "338/338 [==============================] - 15s 43ms/step - loss: 6.8926 - accuracy: 0.0678\n",
      "Epoch 64/64\n",
      "338/338 [==============================] - 15s 44ms/step - loss: 6.8914 - accuracy: 0.0676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f448eca4a90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_set, train_labels, epochs=64, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2704/2704 [==============================] - 15s 5ms/step - loss: 6.8337 - accuracy: 0.0683\n",
      "train loss, train acc: [6.8337321281433105, 0.06833762675523758]\n"
     ]
    }
   ],
   "source": [
    "print(\"train loss, train acc:\", model.evaluate(train_set, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988/988 [==============================] - 5s 5ms/step - loss: 17.0569 - accuracy: 0.0648\n",
      "test loss, test acc: [17.056943893432617, 0.06476937979459763]\n"
     ]
    }
   ],
   "source": [
    "print(\"test loss, test acc:\", model.evaluate(test_set, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch size = 128:  \n",
    "test loss, test acc: [15.796355247497559, 0.0679350420832634] => 50 epochs  \n",
    "test loss, test acc: [18.180967330932617, 0.06521257758140564] => 100 epochs\n",
    "\n",
    "batch size = 256:  \n",
    "test loss, test acc: [21.200485229492188, 0.071987085044384] => 256 epochs\n",
    "test loss, test acc: [28.130123138427734, 0.07205040007829666] => 1024 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xkadlec6/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"phonemes\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=[\"phonemes\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels, label=[\"phonemes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86497, 3)\n",
      "(86497,)\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86497, 33990)\n"
     ]
    }
   ],
   "source": [
    "train_labels = to_categorical(train_labels, num_classes=total_unique_phonemes)\n",
    "test_labels = to_categorical(test_labels, num_classes=total_unique_phonemes)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 33990)             8735430   \n",
      "=================================================================\n",
      "Total params: 8,868,806\n",
      "Trainable params: 8,868,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=512, activation='relu', input_dim=3*1))  # first hidden layer\n",
    "model.add(Dense(units=256, activation='relu'))  # second hidden layer\n",
    "# model.add(Dense(units=128, activation='relu'))  # third hidden layer\n",
    "model.add(Dense(units=total_unique_phonemes, activation='softmax'))  # output layer\n",
    "# model.add(Dense(units=128))  # output layer\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "338/338 [==============================] - 31s 89ms/step - loss: 9.3770 - accuracy: 0.0408\n",
      "Epoch 2/64\n",
      "338/338 [==============================] - 31s 93ms/step - loss: 8.5153 - accuracy: 0.0461\n",
      "Epoch 3/64\n",
      "338/338 [==============================] - 36s 106ms/step - loss: 8.2819 - accuracy: 0.0465\n",
      "Epoch 4/64\n",
      "338/338 [==============================] - 36s 108ms/step - loss: 8.1345 - accuracy: 0.0460\n",
      "Epoch 5/64\n",
      "338/338 [==============================] - 36s 106ms/step - loss: 8.0538 - accuracy: 0.0471\n",
      "Epoch 6/64\n",
      "338/338 [==============================] - 36s 106ms/step - loss: 7.9485 - accuracy: 0.0470\n",
      "Epoch 7/64\n",
      "338/338 [==============================] - 36s 105ms/step - loss: 7.8536 - accuracy: 0.0486\n",
      "Epoch 8/64\n",
      "338/338 [==============================] - 36s 106ms/step - loss: 7.8113 - accuracy: 0.0463\n",
      "Epoch 9/64\n",
      "338/338 [==============================] - 36s 106ms/step - loss: 7.7510 - accuracy: 0.0490\n",
      "Epoch 10/64\n",
      "338/338 [==============================] - 36s 105ms/step - loss: 7.7258 - accuracy: 0.0484\n",
      "Epoch 11/64\n",
      "338/338 [==============================] - 36s 106ms/step - loss: 7.6668 - accuracy: 0.0498\n",
      "Epoch 12/64\n",
      "338/338 [==============================] - 36s 105ms/step - loss: 7.6306 - accuracy: 0.0492\n",
      "Epoch 13/64\n",
      "338/338 [==============================] - 36s 106ms/step - loss: 7.6177 - accuracy: 0.0496\n",
      "Epoch 14/64\n",
      "338/338 [==============================] - 36s 107ms/step - loss: 7.5707 - accuracy: 0.0496\n",
      "Epoch 15/64\n",
      "338/338 [==============================] - 36s 108ms/step - loss: 7.5442 - accuracy: 0.0490\n",
      "Epoch 16/64\n",
      "338/338 [==============================] - 37s 109ms/step - loss: 7.5294 - accuracy: 0.0492\n",
      "Epoch 17/64\n",
      "338/338 [==============================] - 37s 108ms/step - loss: 7.4846 - accuracy: 0.0491\n",
      "Epoch 18/64\n",
      "338/338 [==============================] - 36s 108ms/step - loss: 7.4714 - accuracy: 0.0498\n",
      "Epoch 19/64\n",
      "338/338 [==============================] - 37s 108ms/step - loss: 7.4451 - accuracy: 0.0501\n",
      "Epoch 20/64\n",
      "338/338 [==============================] - 37s 108ms/step - loss: 7.4180 - accuracy: 0.0499\n",
      "Epoch 21/64\n",
      "338/338 [==============================] - 36s 107ms/step - loss: 7.4158 - accuracy: 0.0495\n",
      "Epoch 22/64\n",
      "338/338 [==============================] - 36s 108ms/step - loss: 7.3760 - accuracy: 0.0503\n",
      "Epoch 23/64\n",
      "338/338 [==============================] - 36s 107ms/step - loss: 7.3504 - accuracy: 0.0510\n",
      "Epoch 24/64\n",
      "338/338 [==============================] - 36s 107ms/step - loss: 7.3408 - accuracy: 0.0505\n",
      "Epoch 25/64\n",
      "338/338 [==============================] - 36s 105ms/step - loss: 7.3288 - accuracy: 0.0509\n",
      "Epoch 26/64\n",
      "338/338 [==============================] - 35s 104ms/step - loss: 7.3222 - accuracy: 0.0512\n",
      "Epoch 27/64\n",
      "338/338 [==============================] - 35s 105ms/step - loss: 7.3025 - accuracy: 0.0513\n",
      "Epoch 28/64\n",
      "338/338 [==============================] - 36s 105ms/step - loss: 7.2845 - accuracy: 0.0519\n",
      "Epoch 29/64\n",
      "338/338 [==============================] - 37s 108ms/step - loss: 7.2752 - accuracy: 0.0512\n",
      "Epoch 30/64\n",
      "338/338 [==============================] - 37s 109ms/step - loss: 7.2595 - accuracy: 0.0509\n",
      "Epoch 31/64\n",
      "338/338 [==============================] - 36s 105ms/step - loss: 7.2456 - accuracy: 0.0521\n",
      "Epoch 32/64\n",
      "338/338 [==============================] - 36s 107ms/step - loss: 7.2242 - accuracy: 0.0526\n",
      "Epoch 33/64\n",
      "338/338 [==============================] - 37s 108ms/step - loss: 7.2242 - accuracy: 0.0520\n",
      "Epoch 34/64\n",
      "338/338 [==============================] - 36s 107ms/step - loss: 7.2269 - accuracy: 0.0494\n",
      "Epoch 35/64\n",
      "338/338 [==============================] - 36s 107ms/step - loss: 7.2140 - accuracy: 0.0517\n",
      "Epoch 36/64\n",
      "338/338 [==============================] - 37s 108ms/step - loss: 7.2085 - accuracy: 0.0502\n",
      "Epoch 37/64\n",
      "338/338 [==============================] - 37s 108ms/step - loss: 7.1799 - accuracy: 0.0531\n",
      "Epoch 38/64\n",
      "338/338 [==============================] - 36s 108ms/step - loss: 7.1837 - accuracy: 0.0511\n",
      "Epoch 39/64\n",
      "338/338 [==============================] - 36s 107ms/step - loss: 7.1715 - accuracy: 0.0516\n",
      "Epoch 40/64\n",
      "338/338 [==============================] - 36s 107ms/step - loss: 7.1660 - accuracy: 0.0517\n",
      "Epoch 41/64\n",
      "338/338 [==============================] - 36s 106ms/step - loss: 7.1606 - accuracy: 0.0519\n",
      "Epoch 42/64\n",
      "338/338 [==============================] - 36s 106ms/step - loss: 7.1371 - accuracy: 0.0529\n",
      "Epoch 43/64\n",
      "338/338 [==============================] - 36s 105ms/step - loss: 7.1368 - accuracy: 0.0524\n",
      "Epoch 44/64\n",
      "338/338 [==============================] - 35s 105ms/step - loss: 7.1228 - accuracy: 0.0525\n",
      "Epoch 45/64\n",
      "338/338 [==============================] - 36s 105ms/step - loss: 7.1290 - accuracy: 0.0522\n",
      "Epoch 46/64\n",
      "338/338 [==============================] - 36s 107ms/step - loss: 7.1142 - accuracy: 0.0518\n",
      "Epoch 47/64\n",
      "338/338 [==============================] - 36s 106ms/step - loss: 7.1031 - accuracy: 0.0521\n",
      "Epoch 48/64\n",
      "338/338 [==============================] - 37s 108ms/step - loss: 7.1118 - accuracy: 0.0508\n",
      "Epoch 49/64\n",
      "338/338 [==============================] - 36s 106ms/step - loss: 7.1019 - accuracy: 0.0523\n",
      "Epoch 50/64\n",
      "338/338 [==============================] - 36s 107ms/step - loss: 7.0853 - accuracy: 0.0513\n",
      "Epoch 51/64\n",
      "338/338 [==============================] - 37s 109ms/step - loss: 7.0843 - accuracy: 0.0508\n",
      "Epoch 52/64\n",
      "338/338 [==============================] - 36s 106ms/step - loss: 7.0871 - accuracy: 0.0520\n",
      "Epoch 53/64\n",
      "338/338 [==============================] - 36s 105ms/step - loss: 7.0770 - accuracy: 0.0533\n",
      "Epoch 54/64\n",
      "338/338 [==============================] - 36s 105ms/step - loss: 7.0719 - accuracy: 0.0518\n",
      "Epoch 55/64\n",
      "338/338 [==============================] - 36s 105ms/step - loss: 7.0585 - accuracy: 0.0520\n",
      "Epoch 56/64\n",
      "338/338 [==============================] - 36s 107ms/step - loss: 7.0670 - accuracy: 0.0517\n",
      "Epoch 57/64\n",
      "338/338 [==============================] - 36s 106ms/step - loss: 7.0498 - accuracy: 0.0536\n",
      "Epoch 58/64\n",
      "338/338 [==============================] - 36s 105ms/step - loss: 7.0474 - accuracy: 0.0529\n",
      "Epoch 59/64\n",
      "338/338 [==============================] - 35s 105ms/step - loss: 7.0390 - accuracy: 0.0531\n",
      "Epoch 60/64\n",
      "338/338 [==============================] - 35s 104ms/step - loss: 7.0308 - accuracy: 0.0532\n",
      "Epoch 61/64\n",
      "338/338 [==============================] - 35s 104ms/step - loss: 7.0279 - accuracy: 0.0519\n",
      "Epoch 62/64\n",
      "338/338 [==============================] - 35s 104ms/step - loss: 7.0479 - accuracy: 0.0511\n",
      "Epoch 63/64\n",
      "338/338 [==============================] - 35s 105ms/step - loss: 7.0216 - accuracy: 0.0553\n",
      "Epoch 64/64\n",
      "338/338 [==============================] - 34s 101ms/step - loss: 7.0195 - accuracy: 0.0519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9ff8380f98>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_set, train_labels, epochs=64, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2704/2704 [==============================] - 26s 9ms/step - loss: 6.9437 - accuracy: 0.0529\n",
      "train loss, train acc: [6.943676471710205, 0.052868884056806564]\n"
     ]
    }
   ],
   "source": [
    "print(\"train loss, train acc:\", model.evaluate(train_set, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988/988 [==============================] - 9s 9ms/step - loss: 16.9039 - accuracy: 0.0469\n",
      "test loss, test acc: [16.903888702392578, 0.0468517504632473]\n"
     ]
    }
   ],
   "source": [
    "print(\"test loss, test acc:\", model.evaluate(test_set, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size 256:  \n",
    "test loss, test acc: [9.716291427612305, 0.04188166931271553] => 4 epochs  \n",
    "test loss, test acc: [20.68207550048828, 0.049479249864816666] => 128 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import SpatialDropout1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 256)               266240    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 21317)             5478469   \n",
      "=================================================================\n",
      "Total params: 5,810,501\n",
      "Trainable params: 5,810,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#more elaborate model\n",
    "model_lstm = Sequential()\n",
    "\n",
    "#model_lstm.add(Embedding(input_dim = 3, output_dim = 2, input_length = 86497))\n",
    "#model_lstm.add(SpatialDropout1D(0.3))\n",
    "model_lstm.add(LSTM(256, input_shape = (1, 3), dropout = 0.3, recurrent_dropout = 0.3))\n",
    "model_lstm.add(Dense(256, activation = 'relu'))\n",
    "model_lstm.add(Dropout(0.3))\n",
    "model_lstm.add(Dense(total_unique_words, activation = 'softmax'))\n",
    "\n",
    "model_lstm.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xkadlec6/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"words\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=[\"words\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels, label=[\"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86497, 21317)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels, num_classes=total_unique_words)\n",
    "test_labels = to_categorical(test_labels, num_classes=total_unique_words)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86497, 1, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.values.reshape(-1,1,3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, 227, 380])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_values = train_set.values.reshape(-1, 1, 3)\n",
    "reshaped_values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "338/338 [==============================] - 27s 63ms/step - loss: 8.7761 - accuracy: 0.0461\n",
      "Epoch 2/64\n",
      "338/338 [==============================] - 21s 63ms/step - loss: 8.1032 - accuracy: 0.0498\n",
      "Epoch 3/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 8.0558 - accuracy: 0.0504\n",
      "Epoch 4/64\n",
      "338/338 [==============================] - 22s 64ms/step - loss: 8.0206 - accuracy: 0.0495\n",
      "Epoch 5/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.9760 - accuracy: 0.0512\n",
      "Epoch 6/64\n",
      "338/338 [==============================] - 22s 64ms/step - loss: 7.9351 - accuracy: 0.0528\n",
      "Epoch 7/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.9154 - accuracy: 0.0517\n",
      "Epoch 8/64\n",
      "338/338 [==============================] - 20s 60ms/step - loss: 7.9216 - accuracy: 0.0500\n",
      "Epoch 9/64\n",
      "338/338 [==============================] - 21s 63ms/step - loss: 7.8786 - accuracy: 0.0515\n",
      "Epoch 10/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.8778 - accuracy: 0.0512\n",
      "Epoch 11/64\n",
      "338/338 [==============================] - 21s 63ms/step - loss: 7.8489 - accuracy: 0.0518\n",
      "Epoch 12/64\n",
      "338/338 [==============================] - 20s 60ms/step - loss: 7.8365 - accuracy: 0.0533\n",
      "Epoch 13/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.8215 - accuracy: 0.0522\n",
      "Epoch 14/64\n",
      "338/338 [==============================] - 21s 63ms/step - loss: 7.8159 - accuracy: 0.0517\n",
      "Epoch 15/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.7805 - accuracy: 0.0543\n",
      "Epoch 16/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.7699 - accuracy: 0.0530\n",
      "Epoch 17/64\n",
      "338/338 [==============================] - 21s 63ms/step - loss: 7.7593 - accuracy: 0.0533\n",
      "Epoch 18/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.7588 - accuracy: 0.0524\n",
      "Epoch 19/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.7345 - accuracy: 0.0533\n",
      "Epoch 20/64\n",
      "338/338 [==============================] - 20s 60ms/step - loss: 7.7351 - accuracy: 0.0517\n",
      "Epoch 21/64\n",
      "338/338 [==============================] - 22s 64ms/step - loss: 7.6988 - accuracy: 0.0532\n",
      "Epoch 22/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.6871 - accuracy: 0.0552\n",
      "Epoch 23/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.6911 - accuracy: 0.0525\n",
      "Epoch 24/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.6919 - accuracy: 0.0529\n",
      "Epoch 25/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.6702 - accuracy: 0.0526\n",
      "Epoch 26/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.6785 - accuracy: 0.0509\n",
      "Epoch 27/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.6534 - accuracy: 0.0526\n",
      "Epoch 28/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.6437 - accuracy: 0.0525\n",
      "Epoch 29/64\n",
      "338/338 [==============================] - 20s 60ms/step - loss: 7.6436 - accuracy: 0.0534\n",
      "Epoch 30/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.6177 - accuracy: 0.0542\n",
      "Epoch 31/64\n",
      "338/338 [==============================] - 20s 60ms/step - loss: 7.6235 - accuracy: 0.0530\n",
      "Epoch 32/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.6188 - accuracy: 0.0528\n",
      "Epoch 33/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.5947 - accuracy: 0.0539\n",
      "Epoch 34/64\n",
      "338/338 [==============================] - 20s 59ms/step - loss: 7.6015 - accuracy: 0.0525\n",
      "Epoch 35/64\n",
      "338/338 [==============================] - 21s 63ms/step - loss: 7.5877 - accuracy: 0.0544\n",
      "Epoch 36/64\n",
      "338/338 [==============================] - 20s 60ms/step - loss: 7.5662 - accuracy: 0.0534\n",
      "Epoch 37/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.5705 - accuracy: 0.0542\n",
      "Epoch 38/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.5665 - accuracy: 0.0532\n",
      "Epoch 39/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.5489 - accuracy: 0.0545\n",
      "Epoch 40/64\n",
      "338/338 [==============================] - 20s 61ms/step - loss: 7.5535 - accuracy: 0.0527\n",
      "Epoch 41/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.5492 - accuracy: 0.0540\n",
      "Epoch 42/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.5359 - accuracy: 0.0554\n",
      "Epoch 43/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.5325 - accuracy: 0.0528\n",
      "Epoch 44/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.5274 - accuracy: 0.0523\n",
      "Epoch 45/64\n",
      "338/338 [==============================] - 20s 61ms/step - loss: 7.5268 - accuracy: 0.0523\n",
      "Epoch 46/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.5119 - accuracy: 0.0553\n",
      "Epoch 47/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.4979 - accuracy: 0.0531\n",
      "Epoch 48/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.4928 - accuracy: 0.0536\n",
      "Epoch 49/64\n",
      "338/338 [==============================] - 21s 63ms/step - loss: 7.4905 - accuracy: 0.0537\n",
      "Epoch 50/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.5003 - accuracy: 0.0526\n",
      "Epoch 51/64\n",
      "338/338 [==============================] - 20s 60ms/step - loss: 7.4814 - accuracy: 0.0538\n",
      "Epoch 52/64\n",
      "338/338 [==============================] - 22s 64ms/step - loss: 7.4770 - accuracy: 0.0541\n",
      "Epoch 53/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.4880 - accuracy: 0.0526\n",
      "Epoch 54/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.4678 - accuracy: 0.0531\n",
      "Epoch 55/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.4562 - accuracy: 0.0537\n",
      "Epoch 56/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.4620 - accuracy: 0.0526\n",
      "Epoch 57/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.4613 - accuracy: 0.0532\n",
      "Epoch 58/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.4574 - accuracy: 0.0538\n",
      "Epoch 59/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.4423 - accuracy: 0.0535\n",
      "Epoch 60/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.4330 - accuracy: 0.0543\n",
      "Epoch 61/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.4492 - accuracy: 0.0538\n",
      "Epoch 62/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.4254 - accuracy: 0.0554\n",
      "Epoch 63/64\n",
      "338/338 [==============================] - 21s 61ms/step - loss: 7.4211 - accuracy: 0.0530\n",
      "Epoch 64/64\n",
      "338/338 [==============================] - 21s 62ms/step - loss: 7.4205 - accuracy: 0.0533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9ff7fdda58>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.fit(reshaped_values, train_labels, epochs=64, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988/988 [==============================] - 8s 7ms/step - loss: 14.5705 - accuracy: 0.0550\n",
      "test loss, test acc: [14.57046890258789, 0.05498749390244484]\n"
     ]
    }
   ],
   "source": [
    "print(\"test loss, test acc:\", model_lstm.evaluate(test_set.values.reshape(-1, 1, 3), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2704/2704 [==============================] - 19s 7ms/step - loss: 7.0886 - accuracy: 0.0580\n",
      "train loss, train acc: [7.088553428649902, 0.05795576795935631]\n"
     ]
    }
   ],
   "source": [
    "print(\"train loss, train acc:\", model_lstm.evaluate(reshaped_values, train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are really underwhelming and I have no clue why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import SpatialDropout1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 256)               266240    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 33990)             8735430   \n",
      "=================================================================\n",
      "Total params: 9,067,462\n",
      "Trainable params: 9,067,462\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#more elaborate model\n",
    "model_lstm = Sequential()\n",
    "\n",
    "#model_lstm.add(Embedding(input_dim = 3, output_dim = 2, input_length = 86497))\n",
    "#model_lstm.add(SpatialDropout1D(0.3))\n",
    "model_lstm.add(LSTM(256, input_shape = (1, 3), dropout = 0.3, recurrent_dropout = 0.3))\n",
    "model_lstm.add(Dense(256, activation = 'relu'))\n",
    "model_lstm.add(Dropout(0.3))\n",
    "model_lstm.add(Dense(total_unique_phonemes, activation = 'softmax'))\n",
    "\n",
    "model_lstm.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"phonemes\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=[\"phonemes\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels, label=[\"phonemes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86497, 33990)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels, num_classes=total_unique_phonemes)\n",
    "test_labels = to_categorical(test_labels, num_classes=total_unique_phonemes)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, 227, 380])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_values = train_set.values.reshape(-1, 1, 3)\n",
    "reshaped_values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "338/338 [==============================] - 33s 90ms/step - loss: 9.4609 - accuracy: 0.0422\n",
      "Epoch 2/64\n",
      "338/338 [==============================] - 33s 98ms/step - loss: 8.8229 - accuracy: 0.0429\n",
      "Epoch 3/64\n",
      "338/338 [==============================] - 33s 98ms/step - loss: 8.7584 - accuracy: 0.0451\n",
      "Epoch 4/64\n",
      "338/338 [==============================] - 32s 96ms/step - loss: 8.7248 - accuracy: 0.0457\n",
      "Epoch 5/64\n",
      "338/338 [==============================] - 32s 96ms/step - loss: 8.7000 - accuracy: 0.0455\n",
      "Epoch 6/64\n",
      "338/338 [==============================] - 32s 95ms/step - loss: 8.6762 - accuracy: 0.0462\n",
      "Epoch 7/64\n",
      "338/338 [==============================] - 32s 95ms/step - loss: 8.6408 - accuracy: 0.0447\n",
      "Epoch 8/64\n",
      "338/338 [==============================] - 33s 97ms/step - loss: 8.5873 - accuracy: 0.0473\n",
      "Epoch 9/64\n",
      "338/338 [==============================] - 32s 95ms/step - loss: 8.5664 - accuracy: 0.0457\n",
      "Epoch 10/64\n",
      "338/338 [==============================] - 32s 94ms/step - loss: 8.5169 - accuracy: 0.0465\n",
      "Epoch 11/64\n",
      "338/338 [==============================] - 31s 93ms/step - loss: 8.5126 - accuracy: 0.0465\n",
      "Epoch 12/64\n",
      "338/338 [==============================] - 32s 95ms/step - loss: 8.4532 - accuracy: 0.0472\n",
      "Epoch 13/64\n",
      "338/338 [==============================] - 32s 95ms/step - loss: 8.4335 - accuracy: 0.0458\n",
      "Epoch 14/64\n",
      "338/338 [==============================] - 32s 95ms/step - loss: 8.3992 - accuracy: 0.0471\n",
      "Epoch 15/64\n",
      "338/338 [==============================] - 33s 97ms/step - loss: 8.3858 - accuracy: 0.0461\n",
      "Epoch 16/64\n",
      "338/338 [==============================] - 34s 100ms/step - loss: 8.3582 - accuracy: 0.0458\n",
      "Epoch 17/64\n",
      "338/338 [==============================] - 33s 97ms/step - loss: 8.3400 - accuracy: 0.0470\n",
      "Epoch 18/64\n",
      "338/338 [==============================] - 31s 93ms/step - loss: 8.3177 - accuracy: 0.0466\n",
      "Epoch 19/64\n",
      "338/338 [==============================] - 31s 92ms/step - loss: 8.3036 - accuracy: 0.0455\n",
      "Epoch 20/64\n",
      "338/338 [==============================] - 32s 94ms/step - loss: 8.2828 - accuracy: 0.0464\n",
      "Epoch 21/64\n",
      "338/338 [==============================] - 32s 96ms/step - loss: 8.2808 - accuracy: 0.0456\n",
      "Epoch 22/64\n",
      "338/338 [==============================] - 32s 94ms/step - loss: 8.2682 - accuracy: 0.0447\n",
      "Epoch 23/64\n",
      "338/338 [==============================] - 31s 92ms/step - loss: 8.2280 - accuracy: 0.0465\n",
      "Epoch 24/64\n",
      "338/338 [==============================] - 32s 93ms/step - loss: 8.2061 - accuracy: 0.0469\n",
      "Epoch 25/64\n",
      "338/338 [==============================] - 32s 95ms/step - loss: 8.2072 - accuracy: 0.0445\n",
      "Epoch 26/64\n",
      "338/338 [==============================] - 32s 95ms/step - loss: 8.2096 - accuracy: 0.0450\n",
      "Epoch 27/64\n",
      "338/338 [==============================] - 33s 97ms/step - loss: 8.1941 - accuracy: 0.0467\n",
      "Epoch 28/64\n",
      "338/338 [==============================] - 33s 97ms/step - loss: 8.1513 - accuracy: 0.0484\n",
      "Epoch 29/64\n",
      "338/338 [==============================] - 33s 97ms/step - loss: 8.1430 - accuracy: 0.0460\n",
      "Epoch 30/64\n",
      "338/338 [==============================] - 32s 96ms/step - loss: 8.1298 - accuracy: 0.0477\n",
      "Epoch 31/64\n",
      "338/338 [==============================] - 33s 97ms/step - loss: 8.1317 - accuracy: 0.0458\n",
      "Epoch 32/64\n",
      "338/338 [==============================] - 33s 97ms/step - loss: 8.1166 - accuracy: 0.0467\n",
      "Epoch 33/64\n",
      "338/338 [==============================] - 32s 94ms/step - loss: 8.1129 - accuracy: 0.0454\n",
      "Epoch 34/64\n",
      "338/338 [==============================] - 31s 91ms/step - loss: 8.1018 - accuracy: 0.0464\n",
      "Epoch 35/64\n",
      "338/338 [==============================] - 31s 93ms/step - loss: 8.0866 - accuracy: 0.0458\n",
      "Epoch 36/64\n",
      "338/338 [==============================] - 32s 93ms/step - loss: 8.0656 - accuracy: 0.0461\n",
      "Epoch 37/64\n",
      "338/338 [==============================] - 32s 95ms/step - loss: 8.0683 - accuracy: 0.0463\n",
      "Epoch 38/64\n",
      "338/338 [==============================] - 32s 95ms/step - loss: 8.0369 - accuracy: 0.0457\n",
      "Epoch 39/64\n",
      "338/338 [==============================] - 32s 95ms/step - loss: 8.0533 - accuracy: 0.0449\n",
      "Epoch 40/64\n",
      "338/338 [==============================] - 32s 96ms/step - loss: 8.0249 - accuracy: 0.04720s - loss: 8.0\n",
      "Epoch 41/64\n",
      "338/338 [==============================] - 32s 96ms/step - loss: 8.0193 - accuracy: 0.0462\n",
      "Epoch 42/64\n",
      "338/338 [==============================] - 32s 96ms/step - loss: 8.0076 - accuracy: 0.0466\n",
      "Epoch 43/64\n",
      "338/338 [==============================] - 32s 94ms/step - loss: 7.9950 - accuracy: 0.0455\n",
      "Epoch 44/64\n",
      "338/338 [==============================] - 32s 96ms/step - loss: 7.9825 - accuracy: 0.0457\n",
      "Epoch 45/64\n",
      "338/338 [==============================] - 32s 93ms/step - loss: 7.9917 - accuracy: 0.0457\n",
      "Epoch 46/64\n",
      "338/338 [==============================] - 32s 95ms/step - loss: 7.9662 - accuracy: 0.0448\n",
      "Epoch 47/64\n",
      "338/338 [==============================] - 33s 97ms/step - loss: 7.9643 - accuracy: 0.0466\n",
      "Epoch 48/64\n",
      "338/338 [==============================] - 32s 96ms/step - loss: 7.9555 - accuracy: 0.0465\n",
      "Epoch 49/64\n",
      "338/338 [==============================] - 33s 98ms/step - loss: 7.9349 - accuracy: 0.0471\n",
      "Epoch 50/64\n",
      "338/338 [==============================] - 34s 100ms/step - loss: 7.9232 - accuracy: 0.0466\n",
      "Epoch 51/64\n",
      "338/338 [==============================] - 32s 95ms/step - loss: 7.9249 - accuracy: 0.0456\n",
      "Epoch 52/64\n",
      "338/338 [==============================] - 32s 95ms/step - loss: 7.9229 - accuracy: 0.0455\n",
      "Epoch 53/64\n",
      "338/338 [==============================] - 32s 95ms/step - loss: 7.9189 - accuracy: 0.0466\n",
      "Epoch 54/64\n",
      "338/338 [==============================] - 32s 94ms/step - loss: 7.9247 - accuracy: 0.0457\n",
      "Epoch 55/64\n",
      "338/338 [==============================] - 32s 95ms/step - loss: 7.9135 - accuracy: 0.0459\n",
      "Epoch 56/64\n",
      "338/338 [==============================] - 32s 94ms/step - loss: 7.9087 - accuracy: 0.0457\n",
      "Epoch 57/64\n",
      "338/338 [==============================] - 31s 93ms/step - loss: 7.8905 - accuracy: 0.0469\n",
      "Epoch 58/64\n",
      "338/338 [==============================] - 32s 95ms/step - loss: 7.8985 - accuracy: 0.0450\n",
      "Epoch 59/64\n",
      "338/338 [==============================] - 32s 95ms/step - loss: 7.8847 - accuracy: 0.0459\n",
      "Epoch 60/64\n",
      "338/338 [==============================] - 33s 97ms/step - loss: 7.8631 - accuracy: 0.0474\n",
      "Epoch 61/64\n",
      "338/338 [==============================] - 32s 95ms/step - loss: 7.8569 - accuracy: 0.0469\n",
      "Epoch 62/64\n",
      "338/338 [==============================] - 32s 96ms/step - loss: 7.8666 - accuracy: 0.0462\n",
      "Epoch 63/64\n",
      "338/338 [==============================] - 33s 97ms/step - loss: 7.8565 - accuracy: 0.0459\n",
      "Epoch 64/64\n",
      "338/338 [==============================] - 33s 97ms/step - loss: 7.8312 - accuracy: 0.0464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb182f05400>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.fit(reshaped_values, train_labels, epochs=64, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988/988 [==============================] - 6s 5ms/step - loss: 12.5681 - accuracy: 0.0433\n",
      "test loss, test acc: [12.568110466003418, 0.0433378703892231]\n"
     ]
    }
   ],
   "source": [
    "print(\"test loss, test acc:\", model_lstm.evaluate(test_set.values.reshape(-1, 1, 3), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2704/2704 [==============================] - 15s 6ms/step - loss: 7.4144 - accuracy: 0.0480\n",
      "train loss, train acc: [7.414408206939697, 0.048001665621995926]\n"
     ]
    }
   ],
   "source": [
    "print(\"train loss, train acc:\", model_lstm.evaluate(reshaped_values, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

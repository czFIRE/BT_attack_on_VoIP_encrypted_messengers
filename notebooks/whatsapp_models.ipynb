{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication of attack on VoIP end-to-end encrypted messengers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to explore various models on `Whatsapp` dataset. Bellow we will find loading and preprocessing that we have come up with in the analysis section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "sns.set()  # make plots nicer\n",
    "\n",
    "np.random.seed(42)  # set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_parser_with_prev_next(path):\n",
    "    file = open(path, 'r')\n",
    "    lines = file.readlines()\n",
    "    \n",
    "    file_name = [path.split('/')[-1]]\n",
    "    sentence = \"\"\n",
    "    file_data = []\n",
    "    \n",
    "    has_value = False\n",
    "    previous = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # if there are only 2 informations on line and second is h#, then ignore\n",
    "        # strip line, split primarly on ; secondary on ,\n",
    "        if (line.startswith('#')):\n",
    "            if (not sentence):\n",
    "                sentence = line[len('# Sentence: \"'): len(line) - 1]\n",
    "            continue\n",
    "        \n",
    "        line = line.split(';')\n",
    "        \n",
    "        if (len(line) == 1):\n",
    "            #lines containing only their packet size and nothing else, they should be added\n",
    "            #TODO\n",
    "            line += [\"\"]\n",
    "            line += [\"\"]\n",
    "            #continue\n",
    "        \n",
    "        if (len(line) == 2):\n",
    "            #this tries to remove most of the silence at the start of the recording\n",
    "            #potentionally harmfull as we shouldn't clean test data this way (we will be reading labels)\n",
    "            #if (line[1] == 'h#'):\n",
    "            #    continue\n",
    "            line += [\"\"]\n",
    "        \n",
    "        line[1] = tuple(line[1].split(','))\n",
    "        line[2] = tuple(list(map(lambda a: a.strip('\"'), line[2].split(','))))\n",
    "        \n",
    "        if (has_value):\n",
    "            file_data[-1][4] = line[0]\n",
    "           \n",
    "        # file_type and sentence contain duplicate informations, but are kept for readability\n",
    "        line = file_name + [file_name[0][0:9]] + [sentence] + [previous] + [0] + line\n",
    "        #adding previous as feature\n",
    "        previous = line[5]\n",
    "        file_data += [line]\n",
    "        \n",
    "        #adding next frame as feature\n",
    "        has_value = True\n",
    "        \n",
    "        \n",
    "\n",
    "    return pd.DataFrame(file_data, columns=['file', 'speaker', 'sentence', 'previous_packet', 'next_packet','packet_size', 'phonemes', 'words'])\n",
    "\n",
    "def load_files_with_prev_next(directory):\n",
    "    filelist = os.listdir(directory)\n",
    "    #read them into pandas\n",
    "    df_list = [file_parser_with_prev_next(directory+file) for file in filelist]\n",
    "    #concatenate them together\n",
    "    return pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_types(data_frame):\n",
    "    data_frame['packet_size'] = pd.to_numeric(data_frame['packet_size'])\n",
    "    data_frame['previous_packet'] = pd.to_numeric(data_frame['previous_packet'])\n",
    "    data_frame['next_packet'] = pd.to_numeric(data_frame['next_packet'])\n",
    "\n",
    "    data_frame['file'] = data_frame['file'].astype('category')\n",
    "    data_frame['sentence'] = data_frame['sentence'].astype('category')\n",
    "    data_frame['speaker'] = data_frame['speaker'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>speaker</th>\n",
       "      <th>sentence</th>\n",
       "      <th>previous_packet</th>\n",
       "      <th>next_packet</th>\n",
       "      <th>packet_size</th>\n",
       "      <th>phonemes</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DR1-FAKS0-SA1.CSV</td>\n",
       "      <td>DR1-FAKS0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>0</td>\n",
       "      <td>342</td>\n",
       "      <td>249</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DR1-FAKS0-SA1.CSV</td>\n",
       "      <td>DR1-FAKS0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>249</td>\n",
       "      <td>335</td>\n",
       "      <td>342</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DR1-FAKS0-SA1.CSV</td>\n",
       "      <td>DR1-FAKS0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>342</td>\n",
       "      <td>303</td>\n",
       "      <td>335</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR1-FAKS0-SA1.CSV</td>\n",
       "      <td>DR1-FAKS0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>335</td>\n",
       "      <td>364</td>\n",
       "      <td>303</td>\n",
       "      <td>(h#, sh)</td>\n",
       "      <td>(she,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DR1-FAKS0-SA1.CSV</td>\n",
       "      <td>DR1-FAKS0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>303</td>\n",
       "      <td>418</td>\n",
       "      <td>364</td>\n",
       "      <td>(sh, iy, hv)</td>\n",
       "      <td>(she, had)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31584</th>\n",
       "      <td>DR8-MSLB0-SX383.CSV</td>\n",
       "      <td>DR8-MSLB0</td>\n",
       "      <td>The carpet cleaners shampooed our oriental rug.</td>\n",
       "      <td>338</td>\n",
       "      <td>370</td>\n",
       "      <td>303</td>\n",
       "      <td>(r, ao, r)</td>\n",
       "      <td>(our, oriental)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31585</th>\n",
       "      <td>DR8-MSLB0-SX383.CSV</td>\n",
       "      <td>DR8-MSLB0</td>\n",
       "      <td>The carpet cleaners shampooed our oriental rug.</td>\n",
       "      <td>303</td>\n",
       "      <td>314</td>\n",
       "      <td>370</td>\n",
       "      <td>(r, iy, eh)</td>\n",
       "      <td>(oriental,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31586</th>\n",
       "      <td>DR8-MSLB0-SX383.CSV</td>\n",
       "      <td>DR8-MSLB0</td>\n",
       "      <td>The carpet cleaners shampooed our oriental rug.</td>\n",
       "      <td>370</td>\n",
       "      <td>303</td>\n",
       "      <td>314</td>\n",
       "      <td>(eh, n, tcl, t)</td>\n",
       "      <td>(oriental,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31587</th>\n",
       "      <td>DR8-MSLB0-SX383.CSV</td>\n",
       "      <td>DR8-MSLB0</td>\n",
       "      <td>The carpet cleaners shampooed our oriental rug.</td>\n",
       "      <td>314</td>\n",
       "      <td>295</td>\n",
       "      <td>303</td>\n",
       "      <td>(t, el, r, ah)</td>\n",
       "      <td>(oriental, rug)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31588</th>\n",
       "      <td>DR8-MSLB0-SX383.CSV</td>\n",
       "      <td>DR8-MSLB0</td>\n",
       "      <td>The carpet cleaners shampooed our oriental rug.</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "      <td>295</td>\n",
       "      <td>(ah, gcl)</td>\n",
       "      <td>(rug,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31589 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      file    speaker  \\\n",
       "0        DR1-FAKS0-SA1.CSV  DR1-FAKS0   \n",
       "1        DR1-FAKS0-SA1.CSV  DR1-FAKS0   \n",
       "2        DR1-FAKS0-SA1.CSV  DR1-FAKS0   \n",
       "3        DR1-FAKS0-SA1.CSV  DR1-FAKS0   \n",
       "4        DR1-FAKS0-SA1.CSV  DR1-FAKS0   \n",
       "...                    ...        ...   \n",
       "31584  DR8-MSLB0-SX383.CSV  DR8-MSLB0   \n",
       "31585  DR8-MSLB0-SX383.CSV  DR8-MSLB0   \n",
       "31586  DR8-MSLB0-SX383.CSV  DR8-MSLB0   \n",
       "31587  DR8-MSLB0-SX383.CSV  DR8-MSLB0   \n",
       "31588  DR8-MSLB0-SX383.CSV  DR8-MSLB0   \n",
       "\n",
       "                                                sentence  previous_packet  \\\n",
       "0      She had your dark suit in greasy wash water al...                0   \n",
       "1      She had your dark suit in greasy wash water al...              249   \n",
       "2      She had your dark suit in greasy wash water al...              342   \n",
       "3      She had your dark suit in greasy wash water al...              335   \n",
       "4      She had your dark suit in greasy wash water al...              303   \n",
       "...                                                  ...              ...   \n",
       "31584    The carpet cleaners shampooed our oriental rug.              338   \n",
       "31585    The carpet cleaners shampooed our oriental rug.              303   \n",
       "31586    The carpet cleaners shampooed our oriental rug.              370   \n",
       "31587    The carpet cleaners shampooed our oriental rug.              314   \n",
       "31588    The carpet cleaners shampooed our oriental rug.              303   \n",
       "\n",
       "       next_packet  packet_size         phonemes            words  \n",
       "0              342          249            (h#,)              (,)  \n",
       "1              335          342            (h#,)              (,)  \n",
       "2              303          335            (h#,)              (,)  \n",
       "3              364          303         (h#, sh)           (she,)  \n",
       "4              418          364     (sh, iy, hv)       (she, had)  \n",
       "...            ...          ...              ...              ...  \n",
       "31584          370          303       (r, ao, r)  (our, oriental)  \n",
       "31585          314          370      (r, iy, eh)      (oriental,)  \n",
       "31586          303          314  (eh, n, tcl, t)      (oriental,)  \n",
       "31587          295          303   (t, el, r, ah)  (oriental, rug)  \n",
       "31588            0          295        (ah, gcl)           (rug,)  \n",
       "\n",
       "[31589 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whatsapp_data_train = load_files_with_prev_next(\"./../data/whatsapp_train_data/\")\n",
    "whatsapp_data_test = load_files_with_prev_next(\"./../data/whatsapp_test_data/\")\n",
    "convert_types(whatsapp_data_train)\n",
    "convert_types(whatsapp_data_test)\n",
    "whatsapp_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>speaker</th>\n",
       "      <th>sentence</th>\n",
       "      <th>previous_packet</th>\n",
       "      <th>next_packet</th>\n",
       "      <th>packet_size</th>\n",
       "      <th>prev_curr</th>\n",
       "      <th>next_curr</th>\n",
       "      <th>packet_surrounding</th>\n",
       "      <th>phonemes</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DR1-FCJF0-SA1.CSV</td>\n",
       "      <td>DR1-FCJF0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>227</td>\n",
       "      <td>(0, 227)</td>\n",
       "      <td>(380, 227)</td>\n",
       "      <td>(0, 227, 380)</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DR1-FCJF0-SA1.CSV</td>\n",
       "      <td>DR1-FCJF0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>227</td>\n",
       "      <td>407</td>\n",
       "      <td>380</td>\n",
       "      <td>(227, 380)</td>\n",
       "      <td>(407, 380)</td>\n",
       "      <td>(227, 380, 407)</td>\n",
       "      <td>(h#, sh, ix)</td>\n",
       "      <td>(she,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DR1-FCJF0-SA1.CSV</td>\n",
       "      <td>DR1-FCJF0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>380</td>\n",
       "      <td>350</td>\n",
       "      <td>407</td>\n",
       "      <td>(380, 407)</td>\n",
       "      <td>(350, 407)</td>\n",
       "      <td>(380, 407, 350)</td>\n",
       "      <td>(ix, hv, eh)</td>\n",
       "      <td>(she, had)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR1-FCJF0-SA1.CSV</td>\n",
       "      <td>DR1-FCJF0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>407</td>\n",
       "      <td>281</td>\n",
       "      <td>350</td>\n",
       "      <td>(407, 350)</td>\n",
       "      <td>(281, 350)</td>\n",
       "      <td>(407, 350, 281)</td>\n",
       "      <td>(eh, dcl, jh)</td>\n",
       "      <td>(had, your)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DR1-FCJF0-SA1.CSV</td>\n",
       "      <td>DR1-FCJF0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>350</td>\n",
       "      <td>327</td>\n",
       "      <td>281</td>\n",
       "      <td>(350, 281)</td>\n",
       "      <td>(327, 281)</td>\n",
       "      <td>(350, 281, 327)</td>\n",
       "      <td>(jh, ih, dcl, d, ah)</td>\n",
       "      <td>(had, your, dark)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86492</th>\n",
       "      <td>DR8-MTCS0-SX82.CSV</td>\n",
       "      <td>DR8-MTCS0</td>\n",
       "      <td>Good service should be rewarded by big tips.</td>\n",
       "      <td>286</td>\n",
       "      <td>253</td>\n",
       "      <td>268</td>\n",
       "      <td>(286, 268)</td>\n",
       "      <td>(253, 268)</td>\n",
       "      <td>(286, 268, 253)</td>\n",
       "      <td>(ay, bcl, b, ih)</td>\n",
       "      <td>(by, big)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86493</th>\n",
       "      <td>DR8-MTCS0-SX82.CSV</td>\n",
       "      <td>DR8-MTCS0</td>\n",
       "      <td>Good service should be rewarded by big tips.</td>\n",
       "      <td>268</td>\n",
       "      <td>315</td>\n",
       "      <td>253</td>\n",
       "      <td>(268, 253)</td>\n",
       "      <td>(315, 253)</td>\n",
       "      <td>(268, 253, 315)</td>\n",
       "      <td>(ih, gcl)</td>\n",
       "      <td>(big,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86494</th>\n",
       "      <td>DR8-MTCS0-SX82.CSV</td>\n",
       "      <td>DR8-MTCS0</td>\n",
       "      <td>Good service should be rewarded by big tips.</td>\n",
       "      <td>253</td>\n",
       "      <td>279</td>\n",
       "      <td>315</td>\n",
       "      <td>(253, 315)</td>\n",
       "      <td>(279, 315)</td>\n",
       "      <td>(253, 315, 279)</td>\n",
       "      <td>(gcl, t, ih)</td>\n",
       "      <td>(big, tips)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86495</th>\n",
       "      <td>DR8-MTCS0-SX82.CSV</td>\n",
       "      <td>DR8-MTCS0</td>\n",
       "      <td>Good service should be rewarded by big tips.</td>\n",
       "      <td>315</td>\n",
       "      <td>392</td>\n",
       "      <td>279</td>\n",
       "      <td>(315, 279)</td>\n",
       "      <td>(392, 279)</td>\n",
       "      <td>(315, 279, 392)</td>\n",
       "      <td>(ih, pcl, p)</td>\n",
       "      <td>(tips,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86496</th>\n",
       "      <td>DR8-MTCS0-SX82.CSV</td>\n",
       "      <td>DR8-MTCS0</td>\n",
       "      <td>Good service should be rewarded by big tips.</td>\n",
       "      <td>279</td>\n",
       "      <td>0</td>\n",
       "      <td>392</td>\n",
       "      <td>(279, 392)</td>\n",
       "      <td>(0, 392)</td>\n",
       "      <td>(279, 392, 0)</td>\n",
       "      <td>(p, s, h#)</td>\n",
       "      <td>(tips,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86497 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     file    speaker  \\\n",
       "0       DR1-FCJF0-SA1.CSV  DR1-FCJF0   \n",
       "1       DR1-FCJF0-SA1.CSV  DR1-FCJF0   \n",
       "2       DR1-FCJF0-SA1.CSV  DR1-FCJF0   \n",
       "3       DR1-FCJF0-SA1.CSV  DR1-FCJF0   \n",
       "4       DR1-FCJF0-SA1.CSV  DR1-FCJF0   \n",
       "...                   ...        ...   \n",
       "86492  DR8-MTCS0-SX82.CSV  DR8-MTCS0   \n",
       "86493  DR8-MTCS0-SX82.CSV  DR8-MTCS0   \n",
       "86494  DR8-MTCS0-SX82.CSV  DR8-MTCS0   \n",
       "86495  DR8-MTCS0-SX82.CSV  DR8-MTCS0   \n",
       "86496  DR8-MTCS0-SX82.CSV  DR8-MTCS0   \n",
       "\n",
       "                                                sentence  previous_packet  \\\n",
       "0      She had your dark suit in greasy wash water al...                0   \n",
       "1      She had your dark suit in greasy wash water al...              227   \n",
       "2      She had your dark suit in greasy wash water al...              380   \n",
       "3      She had your dark suit in greasy wash water al...              407   \n",
       "4      She had your dark suit in greasy wash water al...              350   \n",
       "...                                                  ...              ...   \n",
       "86492       Good service should be rewarded by big tips.              286   \n",
       "86493       Good service should be rewarded by big tips.              268   \n",
       "86494       Good service should be rewarded by big tips.              253   \n",
       "86495       Good service should be rewarded by big tips.              315   \n",
       "86496       Good service should be rewarded by big tips.              279   \n",
       "\n",
       "       next_packet  packet_size   prev_curr   next_curr packet_surrounding  \\\n",
       "0              380          227    (0, 227)  (380, 227)      (0, 227, 380)   \n",
       "1              407          380  (227, 380)  (407, 380)    (227, 380, 407)   \n",
       "2              350          407  (380, 407)  (350, 407)    (380, 407, 350)   \n",
       "3              281          350  (407, 350)  (281, 350)    (407, 350, 281)   \n",
       "4              327          281  (350, 281)  (327, 281)    (350, 281, 327)   \n",
       "...            ...          ...         ...         ...                ...   \n",
       "86492          253          268  (286, 268)  (253, 268)    (286, 268, 253)   \n",
       "86493          315          253  (268, 253)  (315, 253)    (268, 253, 315)   \n",
       "86494          279          315  (253, 315)  (279, 315)    (253, 315, 279)   \n",
       "86495          392          279  (315, 279)  (392, 279)    (315, 279, 392)   \n",
       "86496            0          392  (279, 392)    (0, 392)      (279, 392, 0)   \n",
       "\n",
       "                   phonemes              words  \n",
       "0                     (h#,)                (,)  \n",
       "1              (h#, sh, ix)             (she,)  \n",
       "2              (ix, hv, eh)         (she, had)  \n",
       "3             (eh, dcl, jh)        (had, your)  \n",
       "4      (jh, ih, dcl, d, ah)  (had, your, dark)  \n",
       "...                     ...                ...  \n",
       "86492      (ay, bcl, b, ih)          (by, big)  \n",
       "86493             (ih, gcl)             (big,)  \n",
       "86494          (gcl, t, ih)        (big, tips)  \n",
       "86495          (ih, pcl, p)            (tips,)  \n",
       "86496            (p, s, h#)            (tips,)  \n",
       "\n",
       "[86497 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_surrounding(data_frame):\n",
    "    data_frame['prev_curr'] = list(zip(data_frame.previous_packet, data_frame.packet_size))\n",
    "    data_frame['next_curr'] = list(zip(data_frame.next_packet, data_frame.packet_size))\n",
    "    data_frame['packet_surrounding'] = list(zip(data_frame.previous_packet, data_frame.packet_size, data_frame.next_packet))\n",
    "    \n",
    "    #data_frame['prev_curr'] = data_frame['prev_curr'].astype('category')\n",
    "    #data_frame['next_curr'] = data_frame['next_curr'].astype('category')\n",
    "    #data_frame['packet_surrounding'] = data_frame['packet_surrounding'].astype('category')\n",
    "\n",
    "add_surrounding(whatsapp_data_train)\n",
    "add_surrounding(whatsapp_data_test)\n",
    "\n",
    "whatsapp_data_train = whatsapp_data_train[['file', 'speaker', 'sentence', 'previous_packet', 'next_packet','packet_size', 'prev_curr', 'next_curr', 'packet_surrounding', 'phonemes', 'words']]\n",
    "whatsapp_data_test = whatsapp_data_test[['file', 'speaker', 'sentence', 'previous_packet', 'next_packet','packet_size', 'prev_curr', 'next_curr', 'packet_surrounding', 'phonemes', 'words']]\n",
    "whatsapp_data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add removal of labels for the test_dataset\n",
    "def get_labels(df, label=[\"words\"], feature=[\"previous_packet\", \"packet_size\", \"next_packet\"]):\n",
    "    labels = df[label]\n",
    "    features = df[feature]\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels(train_labels, test_labels, label=[\"words\"]):\n",
    "    train_labels = train_labels.astype('category')\n",
    "    test_labels = test_labels.astype('category')\n",
    "    \n",
    "    total_labels = train_labels.append(test_labels)\n",
    "    \n",
    "    lab_enc = LabelEncoder()\n",
    "    lab_enc.fit(total_labels[label])\n",
    "\n",
    "    train_labels = lab_enc.transform(train_labels[label])\n",
    "    test_labels = lab_enc.transform(test_labels[label])\n",
    "    \n",
    "    return train_labels, test_labels, lab_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16168\n",
      "6739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21317"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train)\n",
    "test_set, test_labels = get_labels(whatsapp_data_test)\n",
    "\n",
    "train_labels = train_labels.astype('category')\n",
    "test_labels = test_labels.astype('category')\n",
    "\n",
    "total_labels = train_labels.append(test_labels)\n",
    "print(len(pd.unique(train_labels.words)))\n",
    "print(len(pd.unique(test_labels.words)))\n",
    "total_unique_words = len(pd.unique(total_labels.words))\n",
    "total_unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that we have a really big problem => there are 5149 new words that we have never seen. As we saw in our analysis we can't really generalise on never seen words before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27369\n",
      "12655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33990"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=['phonemes'])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=['phonemes'])\n",
    "\n",
    "train_labels = train_labels.astype('category')\n",
    "test_labels = test_labels.astype('category')\n",
    "\n",
    "total_labels = train_labels.append(test_labels)\n",
    "print(len(pd.unique(train_labels.phonemes)))\n",
    "print(len(pd.unique(test_labels.phonemes)))\n",
    "total_unique_phonemes = len(pd.unique(total_labels.phonemes))\n",
    "total_unique_phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With phonemes the situation is a bit different, as there are more phonemes and we haven't seen only half of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First model that we will be trying is tree classifier. In the analysis we have noticed, that there is almost a 1:1 correspondence of trigram of phoneme sizes and words (eg. that for every trigram of phoneme sizes there is different word). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train)\n",
    "test_set, test_labels = get_labels(whatsapp_data_test)\n",
    "\n",
    "train_labels = train_labels.astype('category')\n",
    "test_labels = test_labels.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16168\n",
      "6739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21317"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_labels = train_labels.append(test_labels)\n",
    "print(len(pd.unique(train_labels.words)))\n",
    "print(len(pd.unique(test_labels.words)))\n",
    "len(pd.unique(total_labels.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0, 15548, 15578, ...,  2531, 18676, 18676])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_enc = LabelEncoder()\n",
    "lab_enc.fit(total_labels.words)\n",
    "\n",
    "train_labels = lab_enc.transform(train_labels.words)\n",
    "test_labels = lab_enc.transform(test_labels.words)\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"clf\",\n",
    "            DecisionTreeClassifier(criterion=\"entropy\", max_depth=None, splitter=\"best\",\n",
    "                                   min_samples_split=2, random_state=42),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# Words: criterion=\"entropy\", max_depth=None, splitter=\"best\", min_samples_split=2, random_state=42 => 0.97, 0.02\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.2631\n",
      "Test accuracy : 0.0380\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "tree_clf_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "print(f\"Train accuracy: {tree_clf_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {tree_clf_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12858, 13445, 15553, ..., 12837, 14816, 24059])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"phonemes\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=[\"phonemes\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels, label=[\"phonemes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.2454\n",
      "Test accuracy : 0.0293\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "tree_clf_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "print(f\"Train accuracy: {tree_clf_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {tree_clf_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have been able to run these classificators and the best results I was able to get were around 3%, which isn't that good considering KNN was able to get twice that much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look a different kind of classifier => k nearest neighbours. This classifier shouldn't need that much RAM and that much of a computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\swdevelopment\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"words\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=[\"words\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"clf\",\n",
    "            KNeighborsClassifier(20, weights='distance', n_jobs=4)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# train accuracy 0.9723\n",
    "#5   => 0.0313\n",
    "#10  => 0.0377\n",
    "#20  => 0.0450\n",
    "#32  => 0.0497\n",
    "#64  => 0.0567\n",
    "#128 => 0.0625\n",
    "#256 => 0.0668\n",
    "\n",
    "# uniform gives better test results but doesn't seem to be able to \"answer correctly\" on the train test\n",
    "# 64 => 0.0927, 0.0685"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9723\n",
      "Test accuracy : 0.0438\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "knn_clf_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "print(f\"Train accuracy: {knn_clf_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {knn_clf_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the search space of 64 nearest neighbours we get only 5.67% success rate on our test data (which is around 1971 words). I have listed other parameters and their resulting percentages in the comments in the code cell. Also worth noting is that \"StandardScaler\" only worsens our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This whole section is just made as sanity check that we actually get expected results (that is we only guess the words we've already seen and none from which we haven't seen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably remove -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known words:\t 16405\n",
      "Unknown words:\t 15184\n"
     ]
    }
   ],
   "source": [
    "data_test_copy = whatsapp_data_test.copy()\n",
    "\n",
    "column_select = list(map(lambda x: x in list(whatsapp_data_train.words.drop_duplicates()), list(data_test_copy.words)))\n",
    "\n",
    "print(\"Known words:\\t\", column_select.count(True))\n",
    "print(\"Unknown words:\\t\", column_select.count(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_copy = data_test_copy[column_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\swdevelopment\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"words\"])\n",
    "test_set, test_labels = get_labels(data_test_copy, label=[\"words\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.0516\n",
      "Test accuracy : 0.0518\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "# knn_clf_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "# 256, distance => 0.9723, 0.1286 on only \n",
    "\n",
    "print(f\"Train accuracy: {knn_clf_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {knn_clf_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we get that the succes rate on known is around double the ammount on all words (this can be seen from the output of a cell 2 cells above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\swdevelopment\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"words\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test[list(map(lambda x: not x, column_select))], label=[\"words\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9723\n",
      "Test accuracy : 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "#knn_clf_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "print(f\"Train accuracy: {knn_clf_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {knn_clf_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test was only made as \"sanity check\" as it is indeed highly probable that our model wouldn't be able to properly guess on never seen examples of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To here remove -------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try our luck with phonemes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\swdevelopment\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"phonemes\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=[\"phonemes\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels, label=[\"phonemes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"clf\",\n",
    "            KNeighborsClassifier(16, weights='distance', n_jobs=4)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# 5 => 0.0256\n",
    "# 6 => 0.0269\n",
    "# 10 => 0.0299\n",
    "# 20 => 0.0343\n",
    "# 32 => 0.0368\n",
    "# 64 => 0.0404\n",
    "# 128 => 0.0441\n",
    "# 256 => 0.0465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9723\n",
      "Test accuracy : 0.0324\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "knn_clf_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "print(f\"Train accuracy: {knn_clf_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {knn_clf_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that phonemes didn't help us that much and that the results are far worse from those gotten by exploring words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xkadlec6/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"words\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=[\"words\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"clf\",\n",
    "            RandomForestClassifier(max_depth=12, random_state=42, criterion = 'entropy', n_jobs = -1, min_samples_split = 2)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# (max_depth=12, random_state=42, criterion = 'entropy', n_jobs = -1, min_samples_split = 2) => 0.6181, 0.0651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting!\n",
      "Finished!\n",
      "Train accuracy: 0.6181\n",
      "Test accuracy : 0.0651\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "rfc_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "print(f\"Train accuracy: {rfc_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {rfc_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that this indeed has better accuracy than normal tree / KNN, but takes way more system resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xkadlec6/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"words\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=[\"words\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"clf\",\n",
    "            AdaBoostClassifier(random_state=1, n_estimators = 60, learning_rate=0.9)\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0505, 0.0471\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "#abc_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "#print(f\"Train accuracy: {abc_pipeline.score(train_set, train_labels):.4f}\")\n",
    "#print(f\"Test accuracy : {abc_pipeline.score(test_set, test_labels):.4f}\")\n",
    "\n",
    "print(\"0.0505, 0.0471\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier ended in absolute failure as it wasn't able to get even acceptable results on the train data. And it even took 8 hours to learn (this is because it can only use 1 thread), so this classifier is pretty much worthless to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xkadlec6/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"words\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=[\"words\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86497, 3)\n",
      "(86497,)\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86497, 21317)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels, num_classes=total_unique_words)\n",
    "test_labels = to_categorical(test_labels, num_classes=total_unique_words)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 21317)             5478469   \n",
      "=================================================================\n",
      "Total params: 5,611,845\n",
      "Trainable params: 5,611,845\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=512, activation='relu', input_dim=3*1))  # first hidden layer\n",
    "model.add(Dense(units=256, activation='relu'))  # second hidden layer\n",
    "# model.add(Dense(units=128, activation='relu'))  # third hidden layer\n",
    "model.add(Dense(units=total_unique_words, activation='softmax'))  # output layer\n",
    "# model.add(Dense(units=128))  # output layer\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338/338 [==============================] - 13s 37ms/step - loss: 8.7219 - accuracy: 0.0470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbd030e6940>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_set, train_labels, epochs=1, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988/988 [==============================] - 4s 4ms/step - loss: 10.1002 - accuracy: 0.0490\n",
      "test loss, test acc: [10.100208282470703, 0.04897274449467659]\n"
     ]
    }
   ],
   "source": [
    "print(\"test loss, test acc:\", model.evaluate(test_set, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch size = 128:  \n",
    "test loss, test acc: [15.796355247497559, 0.0679350420832634] => 50 epochs  \n",
    "test loss, test acc: [18.180967330932617, 0.06521257758140564] => 100 epochs\n",
    "\n",
    "batch size = 256:  \n",
    "test loss, test acc: [21.200485229492188, 0.071987085044384] => 256 epochs\n",
    "test loss, test acc: [28.130123138427734, 0.07205040007829666] => 1024 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pred_y = model.predict(test_set)\n",
    "print(len(pred_y))\n",
    "print(pred_y[0])\n",
    "\n",
    "pred_y_labels = [0]*len(pred_y)\n",
    "for i in range(len(pred_y)):\n",
    "    pred_y_labels[i] = np.argmax(pred_y[i])\n",
    "    \n",
    "print(pred_y_labels[0])\n",
    "\n",
    "print(classification_report(test_labels, pred_y_labels))\n",
    "\"\"\"\n",
    "print(\"Not used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xkadlec6/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"phonemes\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=[\"phonemes\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels, label=[\"phonemes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86497, 3)\n",
      "(86497,)\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86497, 33990)\n"
     ]
    }
   ],
   "source": [
    "train_labels = to_categorical(train_labels, num_classes=total_unique_phonemes)\n",
    "test_labels = to_categorical(test_labels, num_classes=total_unique_phonemes)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 33990)             8735430   \n",
      "=================================================================\n",
      "Total params: 8,868,806\n",
      "Trainable params: 8,868,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=512, activation='relu', input_dim=3*1))  # first hidden layer\n",
    "model.add(Dense(units=256, activation='relu'))  # second hidden layer\n",
    "# model.add(Dense(units=128, activation='relu'))  # third hidden layer\n",
    "model.add(Dense(units=total_unique_phonemes, activation='softmax'))  # output layer\n",
    "# model.add(Dense(units=128))  # output layer\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "338/338 [==============================] - 37s 108ms/step - loss: 9.4086 - accuracy: 0.0408\n",
      "Epoch 2/128\n",
      "338/338 [==============================] - 37s 108ms/step - loss: 8.5344 - accuracy: 0.0470\n",
      "Epoch 3/128\n",
      "338/338 [==============================] - 36s 107ms/step - loss: 8.2638 - accuracy: 0.0471\n",
      "Epoch 4/128\n",
      "338/338 [==============================] - 36s 106ms/step - loss: 8.1087 - accuracy: 0.0468\n",
      "Epoch 5/128\n",
      "338/338 [==============================] - 35s 105ms/step - loss: 8.0128 - accuracy: 0.0472\n",
      "Epoch 6/128\n",
      "338/338 [==============================] - 36s 106ms/step - loss: 7.9068 - accuracy: 0.0489\n",
      "Epoch 7/128\n",
      "338/338 [==============================] - 35s 105ms/step - loss: 7.8324 - accuracy: 0.0475\n",
      "Epoch 8/128\n",
      "338/338 [==============================] - 35s 103ms/step - loss: 7.7745 - accuracy: 0.0475\n",
      "Epoch 9/128\n",
      "338/338 [==============================] - 36s 105ms/step - loss: 7.7123 - accuracy: 0.0473\n",
      "Epoch 10/128\n",
      "338/338 [==============================] - 35s 104ms/step - loss: 7.6526 - accuracy: 0.0480\n",
      "Epoch 11/128\n",
      "338/338 [==============================] - 35s 102ms/step - loss: 7.6105 - accuracy: 0.0480\n",
      "Epoch 12/128\n",
      "338/338 [==============================] - 36s 107ms/step - loss: 7.5727 - accuracy: 0.0484\n",
      "Epoch 13/128\n",
      "338/338 [==============================] - 36s 106ms/step - loss: 7.5409 - accuracy: 0.0490\n",
      "Epoch 14/128\n",
      "338/338 [==============================] - 36s 106ms/step - loss: 7.5059 - accuracy: 0.0481\n",
      "Epoch 15/128\n",
      "338/338 [==============================] - 36s 106ms/step - loss: 7.4828 - accuracy: 0.0490\n",
      "Epoch 16/128\n",
      "338/338 [==============================] - 35s 105ms/step - loss: 7.4499 - accuracy: 0.0500\n",
      "Epoch 17/128\n",
      "338/338 [==============================] - 35s 105ms/step - loss: 7.4318 - accuracy: 0.0493\n",
      "Epoch 18/128\n",
      "338/338 [==============================] - 36s 105ms/step - loss: 7.3928 - accuracy: 0.0510\n",
      "Epoch 19/128\n",
      "338/338 [==============================] - 36s 107ms/step - loss: 7.3773 - accuracy: 0.0498\n",
      "Epoch 20/128\n",
      "338/338 [==============================] - 36s 105ms/step - loss: 7.3554 - accuracy: 0.0499\n",
      "Epoch 21/128\n",
      "338/338 [==============================] - 35s 105ms/step - loss: 7.3483 - accuracy: 0.0496\n",
      "Epoch 22/128\n",
      "338/338 [==============================] - 31s 92ms/step - loss: 7.3214 - accuracy: 0.0493\n",
      "Epoch 23/128\n",
      "338/338 [==============================] - 31s 91ms/step - loss: 7.3088 - accuracy: 0.0488\n",
      "Epoch 24/128\n",
      "338/338 [==============================] - 31s 90ms/step - loss: 7.2996 - accuracy: 0.0492\n",
      "Epoch 25/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 7.2796 - accuracy: 0.0493\n",
      "Epoch 26/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 7.2617 - accuracy: 0.0505\n",
      "Epoch 27/128\n",
      "338/338 [==============================] - 31s 91ms/step - loss: 7.2431 - accuracy: 0.0510\n",
      "Epoch 28/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 7.2355 - accuracy: 0.0494\n",
      "Epoch 29/128\n",
      "338/338 [==============================] - 31s 90ms/step - loss: 7.2305 - accuracy: 0.0493\n",
      "Epoch 30/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 7.1953 - accuracy: 0.0522\n",
      "Epoch 31/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 7.1800 - accuracy: 0.0526\n",
      "Epoch 32/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 7.1852 - accuracy: 0.0524\n",
      "Epoch 33/128\n",
      "338/338 [==============================] - 31s 90ms/step - loss: 7.1777 - accuracy: 0.0511\n",
      "Epoch 34/128\n",
      "338/338 [==============================] - 29s 87ms/step - loss: 7.1791 - accuracy: 0.0505\n",
      "Epoch 35/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 7.1643 - accuracy: 0.0499\n",
      "Epoch 36/128\n",
      "338/338 [==============================] - 29s 86ms/step - loss: 7.1483 - accuracy: 0.0518\n",
      "Epoch 37/128\n",
      "338/338 [==============================] - 29s 87ms/step - loss: 7.1429 - accuracy: 0.0506\n",
      "Epoch 38/128\n",
      "338/338 [==============================] - 29s 87ms/step - loss: 7.1425 - accuracy: 0.0512\n",
      "Epoch 39/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 7.1326 - accuracy: 0.0500\n",
      "Epoch 40/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 7.1281 - accuracy: 0.0498\n",
      "Epoch 41/128\n",
      "338/338 [==============================] - 29s 87ms/step - loss: 7.1083 - accuracy: 0.0513\n",
      "Epoch 42/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 7.1011 - accuracy: 0.0509\n",
      "Epoch 43/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 7.0999 - accuracy: 0.0523\n",
      "Epoch 44/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 7.1031 - accuracy: 0.0510\n",
      "Epoch 45/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 7.0869 - accuracy: 0.0510\n",
      "Epoch 46/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 7.0778 - accuracy: 0.0523\n",
      "Epoch 47/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 7.0792 - accuracy: 0.0507\n",
      "Epoch 48/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 7.0563 - accuracy: 0.0527\n",
      "Epoch 49/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 7.0613 - accuracy: 0.0516\n",
      "Epoch 50/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 7.0518 - accuracy: 0.0511\n",
      "Epoch 51/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 7.0505 - accuracy: 0.0508\n",
      "Epoch 52/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 7.0558 - accuracy: 0.0519\n",
      "Epoch 53/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 7.0423 - accuracy: 0.0522\n",
      "Epoch 54/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 7.0353 - accuracy: 0.0528\n",
      "Epoch 55/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 7.0282 - accuracy: 0.0517\n",
      "Epoch 56/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 7.0239 - accuracy: 0.0525\n",
      "Epoch 57/128\n",
      "338/338 [==============================] - 31s 91ms/step - loss: 7.0209 - accuracy: 0.0512\n",
      "Epoch 58/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 7.0181 - accuracy: 0.0520\n",
      "Epoch 59/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 7.0085 - accuracy: 0.0524\n",
      "Epoch 60/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 7.0027 - accuracy: 0.0540\n",
      "Epoch 61/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 6.9846 - accuracy: 0.0529\n",
      "Epoch 62/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 7.0017 - accuracy: 0.0524\n",
      "Epoch 63/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 7.0059 - accuracy: 0.0534\n",
      "Epoch 64/128\n",
      "338/338 [==============================] - 31s 90ms/step - loss: 6.9838 - accuracy: 0.0539\n",
      "Epoch 65/128\n",
      "338/338 [==============================] - 29s 87ms/step - loss: 6.9785 - accuracy: 0.0538\n",
      "Epoch 66/128\n",
      "338/338 [==============================] - 29s 87ms/step - loss: 6.9816 - accuracy: 0.0530\n",
      "Epoch 67/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 6.9867 - accuracy: 0.0528\n",
      "Epoch 68/128\n",
      "338/338 [==============================] - 31s 91ms/step - loss: 6.9644 - accuracy: 0.0549\n",
      "Epoch 69/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 6.9774 - accuracy: 0.0515\n",
      "Epoch 70/128\n",
      "338/338 [==============================] - 29s 87ms/step - loss: 6.9845 - accuracy: 0.0520\n",
      "Epoch 71/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 6.9536 - accuracy: 0.0541\n",
      "Epoch 72/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 6.9565 - accuracy: 0.0536\n",
      "Epoch 73/128\n",
      "338/338 [==============================] - 29s 87ms/step - loss: 6.9508 - accuracy: 0.0539\n",
      "Epoch 74/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.9609 - accuracy: 0.0527\n",
      "Epoch 75/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.9571 - accuracy: 0.0520\n",
      "Epoch 76/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.9649 - accuracy: 0.0543\n",
      "Epoch 77/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 6.9542 - accuracy: 0.0543\n",
      "Epoch 78/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.9445 - accuracy: 0.0532\n",
      "Epoch 79/128\n",
      "338/338 [==============================] - 31s 91ms/step - loss: 6.9448 - accuracy: 0.0531\n",
      "Epoch 80/128\n",
      "338/338 [==============================] - 31s 91ms/step - loss: 6.9403 - accuracy: 0.0538\n",
      "Epoch 81/128\n",
      "338/338 [==============================] - 31s 91ms/step - loss: 6.9304 - accuracy: 0.0543\n",
      "Epoch 82/128\n",
      "338/338 [==============================] - 31s 91ms/step - loss: 6.9321 - accuracy: 0.0543\n",
      "Epoch 83/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.9389 - accuracy: 0.0539\n",
      "Epoch 84/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 6.9243 - accuracy: 0.0550\n",
      "Epoch 85/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.9215 - accuracy: 0.0531\n",
      "Epoch 86/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 6.9159 - accuracy: 0.0540\n",
      "Epoch 87/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.9244 - accuracy: 0.0546\n",
      "Epoch 88/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.9267 - accuracy: 0.0532\n",
      "Epoch 89/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 6.9094 - accuracy: 0.0541\n",
      "Epoch 90/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.9080 - accuracy: 0.0532\n",
      "Epoch 91/128\n",
      "338/338 [==============================] - 31s 91ms/step - loss: 6.9041 - accuracy: 0.0539\n",
      "Epoch 92/128\n",
      "338/338 [==============================] - 31s 91ms/step - loss: 6.9229 - accuracy: 0.0537\n",
      "Epoch 93/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 6.8886 - accuracy: 0.0544\n",
      "Epoch 94/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.9115 - accuracy: 0.0528\n",
      "Epoch 95/128\n",
      "338/338 [==============================] - 31s 90ms/step - loss: 6.9007 - accuracy: 0.0549\n",
      "Epoch 96/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 6.9067 - accuracy: 0.0541\n",
      "Epoch 97/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.8937 - accuracy: 0.0546\n",
      "Epoch 98/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 6.8937 - accuracy: 0.0547\n",
      "Epoch 99/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 6.9029 - accuracy: 0.0540\n",
      "Epoch 100/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.8944 - accuracy: 0.0537\n",
      "Epoch 101/128\n",
      "338/338 [==============================] - 31s 91ms/step - loss: 6.8922 - accuracy: 0.0553\n",
      "Epoch 102/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.8922 - accuracy: 0.0550\n",
      "Epoch 103/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.8893 - accuracy: 0.0549\n",
      "Epoch 104/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 6.9073 - accuracy: 0.0542\n",
      "Epoch 105/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 6.8810 - accuracy: 0.0546\n",
      "Epoch 106/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 6.8722 - accuracy: 0.0548\n",
      "Epoch 107/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.8802 - accuracy: 0.0532\n",
      "Epoch 108/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 6.8775 - accuracy: 0.0548\n",
      "Epoch 109/128\n",
      "338/338 [==============================] - 31s 90ms/step - loss: 6.8763 - accuracy: 0.0541\n",
      "Epoch 110/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.8822 - accuracy: 0.0534\n",
      "Epoch 111/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 6.8614 - accuracy: 0.0542\n",
      "Epoch 112/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 6.8878 - accuracy: 0.0533\n",
      "Epoch 113/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.8760 - accuracy: 0.0538\n",
      "Epoch 114/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 6.8727 - accuracy: 0.0533\n",
      "Epoch 115/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 6.8712 - accuracy: 0.0531\n",
      "Epoch 116/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 6.8542 - accuracy: 0.0549\n",
      "Epoch 117/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 6.8674 - accuracy: 0.0555\n",
      "Epoch 118/128\n",
      "338/338 [==============================] - 31s 91ms/step - loss: 6.8705 - accuracy: 0.0523\n",
      "Epoch 119/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.8411 - accuracy: 0.0552\n",
      "Epoch 120/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.8617 - accuracy: 0.0544\n",
      "Epoch 121/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 6.8468 - accuracy: 0.0550\n",
      "Epoch 122/128\n",
      "338/338 [==============================] - 30s 90ms/step - loss: 6.8632 - accuracy: 0.0550\n",
      "Epoch 123/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 6.8395 - accuracy: 0.0557\n",
      "Epoch 124/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 6.8573 - accuracy: 0.0552\n",
      "Epoch 125/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 6.8468 - accuracy: 0.0559\n",
      "Epoch 126/128\n",
      "338/338 [==============================] - 30s 88ms/step - loss: 6.8394 - accuracy: 0.0560\n",
      "Epoch 127/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 6.8343 - accuracy: 0.0557\n",
      "Epoch 128/128\n",
      "338/338 [==============================] - 30s 89ms/step - loss: 6.8469 - accuracy: 0.0554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f18668f1eb8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_set, train_labels, epochs=128, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988/988 [==============================] - 7s 5ms/step - loss: 20.6821 - accuracy: 0.0495\n",
      "test loss, test acc: [20.68207550048828, 0.049479249864816666]\n"
     ]
    }
   ],
   "source": [
    "print(\"test loss, test acc:\", model.evaluate(test_set, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size 256:  \n",
    "test loss, test acc: [9.716291427612305, 0.04188166931271553] => 4 epochs  \n",
    "test loss, test acc: [20.68207550048828, 0.049479249864816666] => 128 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 21317)             5478469   \n",
      "=================================================================\n",
      "Total params: 5,742,661\n",
      "Trainable params: 5,742,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(256, input_shape = (1, 3)))\n",
    "model_lstm.add(Dense(units=total_unique_words))\n",
    "model_lstm.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import SpatialDropout1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 256)               266240    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 21317)             5478469   \n",
      "=================================================================\n",
      "Total params: 5,810,501\n",
      "Trainable params: 5,810,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#more elaborate model\n",
    "model_lstm = Sequential()\n",
    "\n",
    "#model_lstm.add(Embedding(input_dim = 3, output_dim = 2, input_length = 86497))\n",
    "#model_lstm.add(SpatialDropout1D(0.3))\n",
    "model_lstm.add(LSTM(256, input_shape = (1, 3), dropout = 0.3, recurrent_dropout = 0.3))\n",
    "model_lstm.add(Dense(256, activation = 'relu'))\n",
    "model_lstm.add(Dropout(0.3))\n",
    "model_lstm.add(Dense(total_unique_words, activation = 'softmax'))\n",
    "\n",
    "model_lstm.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xkadlec6/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(whatsapp_data_train, label=[\"words\"])\n",
    "test_set, test_labels = get_labels(whatsapp_data_test, label=[\"words\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86497, 21317)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels, num_classes=total_unique_words)\n",
    "test_labels = to_categorical(test_labels, num_classes=total_unique_words)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(train_set.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86497, 1, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.values.reshape(-1,1,3).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, 227, 380])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_values = train_set.values.reshape(-1, 1, 3)\n",
    "reshaped_values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "676/676 [==============================] - 27s 36ms/step - loss: 8.6767 - accuracy: 0.0505\n",
      "Epoch 2/16\n",
      "676/676 [==============================] - 24s 35ms/step - loss: 8.1635 - accuracy: 0.0501\n",
      "Epoch 3/16\n",
      "676/676 [==============================] - 25s 37ms/step - loss: 8.1077 - accuracy: 0.0499\n",
      "Epoch 4/16\n",
      "676/676 [==============================] - 24s 36ms/step - loss: 8.0885 - accuracy: 0.0502\n",
      "Epoch 5/16\n",
      "676/676 [==============================] - 25s 36ms/step - loss: 8.0529 - accuracy: 0.0519\n",
      "Epoch 6/16\n",
      "676/676 [==============================] - 25s 36ms/step - loss: 8.0283 - accuracy: 0.0514\n",
      "Epoch 7/16\n",
      "676/676 [==============================] - 24s 36ms/step - loss: 8.0376 - accuracy: 0.0506\n",
      "Epoch 8/16\n",
      "676/676 [==============================] - 24s 35ms/step - loss: 8.0154 - accuracy: 0.0505\n",
      "Epoch 9/16\n",
      "676/676 [==============================] - 24s 35ms/step - loss: 7.9884 - accuracy: 0.0516\n",
      "Epoch 10/16\n",
      "676/676 [==============================] - 24s 36ms/step - loss: 7.9865 - accuracy: 0.0508\n",
      "Epoch 11/16\n",
      "676/676 [==============================] - 25s 37ms/step - loss: 7.9635 - accuracy: 0.0512\n",
      "Epoch 12/16\n",
      "676/676 [==============================] - 24s 35ms/step - loss: 7.9345 - accuracy: 0.0509\n",
      "Epoch 13/16\n",
      "676/676 [==============================] - 24s 35ms/step - loss: 7.9274 - accuracy: 0.0523\n",
      "Epoch 14/16\n",
      "676/676 [==============================] - 24s 36ms/step - loss: 7.9274 - accuracy: 0.0515\n",
      "Epoch 15/16\n",
      "676/676 [==============================] - 24s 36ms/step - loss: 7.9021 - accuracy: 0.0531\n",
      "Epoch 16/16\n",
      "676/676 [==============================] - 24s 36ms/step - loss: 7.9001 - accuracy: 0.0506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f34c57d42e8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.fit(reshaped_values, train_labels, epochs=16, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988/988 [==============================] - 5s 5ms/step - loss: 12.4183 - accuracy: 0.0488\n",
      "test loss, test acc: [12.41828441619873, 0.04875114932656288]\n"
     ]
    }
   ],
   "source": [
    "print(\"test loss, test acc:\", model_lstm.evaluate(test_set.values.reshape(-1, 1, 3), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are really underwhelming and I have no clue why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model_lstm.predict(reshaped_values[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 21317)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

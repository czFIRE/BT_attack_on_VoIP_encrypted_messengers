{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication of attack on VoIP end-to-end encrypted messengers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to explore various models on `skype` dataset. Bellow we will find loading and preprocessing that we have come up with in the analysis section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "sns.set()  # make plots nicer\n",
    "\n",
    "np.random.seed(42)  # set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_parser_with_prev_next(path):\n",
    "    file = open(path, 'r')\n",
    "    lines = file.readlines()\n",
    "    \n",
    "    file_name = [path.split('/')[-1]]\n",
    "    sentence = \"\"\n",
    "    file_data = []\n",
    "    \n",
    "    has_value = False\n",
    "    previous = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # if there are only 2 informations on line and second is h#, then ignore\n",
    "        # strip line, split primarly on ; secondary on ,\n",
    "        if (line.startswith('#')):\n",
    "            if (not sentence):\n",
    "                sentence = line[len('# Sentence: \"'): len(line) - 1]\n",
    "            continue\n",
    "        \n",
    "        line = line.split(';')\n",
    "        \n",
    "        if (len(line) == 1):\n",
    "            #lines containing only their packet size and nothing else, they should be added\n",
    "            #TODO\n",
    "            line += [\"\"]\n",
    "            line += [\"\"]\n",
    "            #continue\n",
    "        \n",
    "        if (len(line) == 2):\n",
    "            #this tries to remove most of the silence at the start of the recording\n",
    "            #potentionally harmfull as we shouldn't clean test data this way (we will be reading labels)\n",
    "            #if (line[1] == 'h#'):\n",
    "            #    continue\n",
    "            line += [\"\"]\n",
    "        \n",
    "        line[1] = tuple(line[1].split(','))\n",
    "        line[2] = tuple(list(map(lambda a: a.strip('\"'), line[2].split(','))))\n",
    "        \n",
    "        if (has_value):\n",
    "            file_data[-1][4] = line[0]\n",
    "           \n",
    "        # file_type and sentence contain duplicate informations, but are kept for readability\n",
    "        line = file_name + [file_name[0][0:9]] + [sentence] + [previous] + [0] + line\n",
    "        #adding previous as feature\n",
    "        previous = line[5]\n",
    "        file_data += [line]\n",
    "        \n",
    "        #adding next frame as feature\n",
    "        has_value = True\n",
    "        \n",
    "        \n",
    "\n",
    "    return pd.DataFrame(file_data, columns=['file', 'speaker', 'sentence', 'previous_packet', 'next_packet','packet_size', 'phonemes', 'words'])\n",
    "\n",
    "def load_files_with_prev_next(directory):\n",
    "    filelist = os.listdir(directory)\n",
    "    #read them into pandas\n",
    "    df_list = [file_parser_with_prev_next(directory+file) for file in filelist]\n",
    "    #concatenate them together\n",
    "    return pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_types(data_frame):\n",
    "    data_frame['packet_size'] = pd.to_numeric(data_frame['packet_size'])\n",
    "    data_frame['previous_packet'] = pd.to_numeric(data_frame['previous_packet'])\n",
    "    data_frame['next_packet'] = pd.to_numeric(data_frame['next_packet'])\n",
    "\n",
    "    data_frame['file'] = data_frame['file'].astype('category')\n",
    "    data_frame['sentence'] = data_frame['sentence'].astype('category')\n",
    "    data_frame['speaker'] = data_frame['speaker'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>speaker</th>\n",
       "      <th>sentence</th>\n",
       "      <th>previous_packet</th>\n",
       "      <th>next_packet</th>\n",
       "      <th>packet_size</th>\n",
       "      <th>phonemes</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DR1-FAKS0-SA1.CSV</td>\n",
       "      <td>DR1-FAKS0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DR1-FAKS0-SA1.CSV</td>\n",
       "      <td>DR1-FAKS0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>35</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DR1-FAKS0-SA1.CSV</td>\n",
       "      <td>DR1-FAKS0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>35</td>\n",
       "      <td>26</td>\n",
       "      <td>43</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR1-FAKS0-SA1.CSV</td>\n",
       "      <td>DR1-FAKS0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>43</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DR1-FAKS0-SA1.CSV</td>\n",
       "      <td>DR1-FAKS0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>26</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258516</th>\n",
       "      <td>DR8-MSLB0-SX383.CSV</td>\n",
       "      <td>DR8-MSLB0</td>\n",
       "      <td>The carpet cleaners shampooed our oriental rug.</td>\n",
       "      <td>40</td>\n",
       "      <td>43</td>\n",
       "      <td>46</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258517</th>\n",
       "      <td>DR8-MSLB0-SX383.CSV</td>\n",
       "      <td>DR8-MSLB0</td>\n",
       "      <td>The carpet cleaners shampooed our oriental rug.</td>\n",
       "      <td>46</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258518</th>\n",
       "      <td>DR8-MSLB0-SX383.CSV</td>\n",
       "      <td>DR8-MSLB0</td>\n",
       "      <td>The carpet cleaners shampooed our oriental rug.</td>\n",
       "      <td>43</td>\n",
       "      <td>34</td>\n",
       "      <td>41</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258519</th>\n",
       "      <td>DR8-MSLB0-SX383.CSV</td>\n",
       "      <td>DR8-MSLB0</td>\n",
       "      <td>The carpet cleaners shampooed our oriental rug.</td>\n",
       "      <td>41</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258520</th>\n",
       "      <td>DR8-MSLB0-SX383.CSV</td>\n",
       "      <td>DR8-MSLB0</td>\n",
       "      <td>The carpet cleaners shampooed our oriental rug.</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>258521 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file    speaker  \\\n",
       "0         DR1-FAKS0-SA1.CSV  DR1-FAKS0   \n",
       "1         DR1-FAKS0-SA1.CSV  DR1-FAKS0   \n",
       "2         DR1-FAKS0-SA1.CSV  DR1-FAKS0   \n",
       "3         DR1-FAKS0-SA1.CSV  DR1-FAKS0   \n",
       "4         DR1-FAKS0-SA1.CSV  DR1-FAKS0   \n",
       "...                     ...        ...   \n",
       "258516  DR8-MSLB0-SX383.CSV  DR8-MSLB0   \n",
       "258517  DR8-MSLB0-SX383.CSV  DR8-MSLB0   \n",
       "258518  DR8-MSLB0-SX383.CSV  DR8-MSLB0   \n",
       "258519  DR8-MSLB0-SX383.CSV  DR8-MSLB0   \n",
       "258520  DR8-MSLB0-SX383.CSV  DR8-MSLB0   \n",
       "\n",
       "                                                 sentence  previous_packet  \\\n",
       "0       She had your dark suit in greasy wash water al...                0   \n",
       "1       She had your dark suit in greasy wash water al...               30   \n",
       "2       She had your dark suit in greasy wash water al...               35   \n",
       "3       She had your dark suit in greasy wash water al...               43   \n",
       "4       She had your dark suit in greasy wash water al...               26   \n",
       "...                                                   ...              ...   \n",
       "258516    The carpet cleaners shampooed our oriental rug.               40   \n",
       "258517    The carpet cleaners shampooed our oriental rug.               46   \n",
       "258518    The carpet cleaners shampooed our oriental rug.               43   \n",
       "258519    The carpet cleaners shampooed our oriental rug.               41   \n",
       "258520    The carpet cleaners shampooed our oriental rug.               34   \n",
       "\n",
       "        next_packet  packet_size phonemes words  \n",
       "0                35           30    (h#,)   (,)  \n",
       "1                43           35    (h#,)   (,)  \n",
       "2                26           43    (h#,)   (,)  \n",
       "3                30           26    (h#,)   (,)  \n",
       "4                31           30    (h#,)   (,)  \n",
       "...             ...          ...      ...   ...  \n",
       "258516           43           46    (h#,)   (,)  \n",
       "258517           41           43    (h#,)   (,)  \n",
       "258518           34           41    (h#,)   (,)  \n",
       "258519           33           34    (h#,)   (,)  \n",
       "258520            0           33    (h#,)   (,)  \n",
       "\n",
       "[258521 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skype_data_train = load_files_with_prev_next(\"./../data/skype_train_data/\")\n",
    "skype_data_test = load_files_with_prev_next(\"./../data/skype_test_data/\")\n",
    "convert_types(skype_data_train)\n",
    "convert_types(skype_data_test)\n",
    "skype_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>speaker</th>\n",
       "      <th>sentence</th>\n",
       "      <th>previous_packet</th>\n",
       "      <th>next_packet</th>\n",
       "      <th>packet_size</th>\n",
       "      <th>prev_curr</th>\n",
       "      <th>next_curr</th>\n",
       "      <th>packet_surrounding</th>\n",
       "      <th>phonemes</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DR1-FCJF0-SA1.CSV</td>\n",
       "      <td>DR1-FCJF0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>(0, 32)</td>\n",
       "      <td>(32, 32)</td>\n",
       "      <td>(0, 32, 32)</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DR1-FCJF0-SA1.CSV</td>\n",
       "      <td>DR1-FCJF0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>32</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>(32, 32)</td>\n",
       "      <td>(31, 32)</td>\n",
       "      <td>(32, 32, 31)</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DR1-FCJF0-SA1.CSV</td>\n",
       "      <td>DR1-FCJF0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "      <td>31</td>\n",
       "      <td>(32, 31)</td>\n",
       "      <td>(28, 31)</td>\n",
       "      <td>(32, 31, 28)</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR1-FCJF0-SA1.CSV</td>\n",
       "      <td>DR1-FCJF0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>31</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>(31, 28)</td>\n",
       "      <td>(28, 28)</td>\n",
       "      <td>(31, 28, 28)</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DR1-FCJF0-SA1.CSV</td>\n",
       "      <td>DR1-FCJF0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>28</td>\n",
       "      <td>36</td>\n",
       "      <td>28</td>\n",
       "      <td>(28, 28)</td>\n",
       "      <td>(36, 28)</td>\n",
       "      <td>(28, 28, 36)</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707433</th>\n",
       "      <td>DR8-MTCS0-SX82.CSV</td>\n",
       "      <td>DR8-MTCS0</td>\n",
       "      <td>Good service should be rewarded by big tips.</td>\n",
       "      <td>47</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>(47, 32)</td>\n",
       "      <td>(34, 32)</td>\n",
       "      <td>(47, 32, 34)</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707434</th>\n",
       "      <td>DR8-MTCS0-SX82.CSV</td>\n",
       "      <td>DR8-MTCS0</td>\n",
       "      <td>Good service should be rewarded by big tips.</td>\n",
       "      <td>32</td>\n",
       "      <td>39</td>\n",
       "      <td>34</td>\n",
       "      <td>(32, 34)</td>\n",
       "      <td>(39, 34)</td>\n",
       "      <td>(32, 34, 39)</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707435</th>\n",
       "      <td>DR8-MTCS0-SX82.CSV</td>\n",
       "      <td>DR8-MTCS0</td>\n",
       "      <td>Good service should be rewarded by big tips.</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "      <td>39</td>\n",
       "      <td>(34, 39)</td>\n",
       "      <td>(33, 39)</td>\n",
       "      <td>(34, 39, 33)</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707436</th>\n",
       "      <td>DR8-MTCS0-SX82.CSV</td>\n",
       "      <td>DR8-MTCS0</td>\n",
       "      <td>Good service should be rewarded by big tips.</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>(39, 33)</td>\n",
       "      <td>(36, 33)</td>\n",
       "      <td>(39, 33, 36)</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707437</th>\n",
       "      <td>DR8-MTCS0-SX82.CSV</td>\n",
       "      <td>DR8-MTCS0</td>\n",
       "      <td>Good service should be rewarded by big tips.</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>(33, 36)</td>\n",
       "      <td>(0, 36)</td>\n",
       "      <td>(33, 36, 0)</td>\n",
       "      <td>(h#,)</td>\n",
       "      <td>(,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>707438 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      file    speaker  \\\n",
       "0        DR1-FCJF0-SA1.CSV  DR1-FCJF0   \n",
       "1        DR1-FCJF0-SA1.CSV  DR1-FCJF0   \n",
       "2        DR1-FCJF0-SA1.CSV  DR1-FCJF0   \n",
       "3        DR1-FCJF0-SA1.CSV  DR1-FCJF0   \n",
       "4        DR1-FCJF0-SA1.CSV  DR1-FCJF0   \n",
       "...                    ...        ...   \n",
       "707433  DR8-MTCS0-SX82.CSV  DR8-MTCS0   \n",
       "707434  DR8-MTCS0-SX82.CSV  DR8-MTCS0   \n",
       "707435  DR8-MTCS0-SX82.CSV  DR8-MTCS0   \n",
       "707436  DR8-MTCS0-SX82.CSV  DR8-MTCS0   \n",
       "707437  DR8-MTCS0-SX82.CSV  DR8-MTCS0   \n",
       "\n",
       "                                                 sentence  previous_packet  \\\n",
       "0       She had your dark suit in greasy wash water al...                0   \n",
       "1       She had your dark suit in greasy wash water al...               32   \n",
       "2       She had your dark suit in greasy wash water al...               32   \n",
       "3       She had your dark suit in greasy wash water al...               31   \n",
       "4       She had your dark suit in greasy wash water al...               28   \n",
       "...                                                   ...              ...   \n",
       "707433       Good service should be rewarded by big tips.               47   \n",
       "707434       Good service should be rewarded by big tips.               32   \n",
       "707435       Good service should be rewarded by big tips.               34   \n",
       "707436       Good service should be rewarded by big tips.               39   \n",
       "707437       Good service should be rewarded by big tips.               33   \n",
       "\n",
       "        next_packet  packet_size prev_curr next_curr packet_surrounding  \\\n",
       "0                32           32   (0, 32)  (32, 32)        (0, 32, 32)   \n",
       "1                31           32  (32, 32)  (31, 32)       (32, 32, 31)   \n",
       "2                28           31  (32, 31)  (28, 31)       (32, 31, 28)   \n",
       "3                28           28  (31, 28)  (28, 28)       (31, 28, 28)   \n",
       "4                36           28  (28, 28)  (36, 28)       (28, 28, 36)   \n",
       "...             ...          ...       ...       ...                ...   \n",
       "707433           34           32  (47, 32)  (34, 32)       (47, 32, 34)   \n",
       "707434           39           34  (32, 34)  (39, 34)       (32, 34, 39)   \n",
       "707435           33           39  (34, 39)  (33, 39)       (34, 39, 33)   \n",
       "707436           36           33  (39, 33)  (36, 33)       (39, 33, 36)   \n",
       "707437            0           36  (33, 36)   (0, 36)        (33, 36, 0)   \n",
       "\n",
       "       phonemes words  \n",
       "0         (h#,)   (,)  \n",
       "1         (h#,)   (,)  \n",
       "2         (h#,)   (,)  \n",
       "3         (h#,)   (,)  \n",
       "4         (h#,)   (,)  \n",
       "...         ...   ...  \n",
       "707433    (h#,)   (,)  \n",
       "707434    (h#,)   (,)  \n",
       "707435    (h#,)   (,)  \n",
       "707436    (h#,)   (,)  \n",
       "707437    (h#,)   (,)  \n",
       "\n",
       "[707438 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_surrounding(data_frame):\n",
    "    data_frame['prev_curr'] = list(zip(data_frame.previous_packet, data_frame.packet_size))\n",
    "    data_frame['next_curr'] = list(zip(data_frame.next_packet, data_frame.packet_size))\n",
    "    data_frame['packet_surrounding'] = list(zip(data_frame.previous_packet, data_frame.packet_size, data_frame.next_packet))\n",
    "    \n",
    "    #data_frame['prev_curr'] = data_frame['prev_curr'].astype('category')\n",
    "    #data_frame['next_curr'] = data_frame['next_curr'].astype('category')\n",
    "    #data_frame['packet_surrounding'] = data_frame['packet_surrounding'].astype('category')\n",
    "\n",
    "add_surrounding(skype_data_train)\n",
    "add_surrounding(skype_data_test)\n",
    "\n",
    "skype_data_train = skype_data_train[['file', 'speaker', 'sentence', 'previous_packet', 'next_packet','packet_size', 'prev_curr', 'next_curr', 'packet_surrounding', 'phonemes', 'words']]\n",
    "skype_data_test = skype_data_test[['file', 'speaker', 'sentence', 'previous_packet', 'next_packet','packet_size', 'prev_curr', 'next_curr', 'packet_surrounding', 'phonemes', 'words']]\n",
    "skype_data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add removal of labels for the test_dataset\n",
    "def get_labels(df, label=[\"words\"], feature=[\"previous_packet\", \"packet_size\", \"next_packet\"]):\n",
    "    labels = df[label]\n",
    "    features = df[feature]\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels(train_labels, test_labels, label=[\"words\"]):\n",
    "    train_labels = train_labels.astype('category')\n",
    "    test_labels = test_labels.astype('category')\n",
    "    \n",
    "    total_labels = train_labels.append(test_labels)\n",
    "    \n",
    "    lab_enc = LabelEncoder()\n",
    "    lab_enc.fit(total_labels[label])\n",
    "\n",
    "    train_labels = lab_enc.transform(train_labels[label])\n",
    "    test_labels = lab_enc.transform(test_labels[label])\n",
    "    \n",
    "    return train_labels, test_labels, lab_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15713\n",
      "6626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20568"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(skype_data_train)\n",
    "test_set, test_labels = get_labels(skype_data_test)\n",
    "\n",
    "train_labels = train_labels.astype('category')\n",
    "test_labels = test_labels.astype('category')\n",
    "\n",
    "total_labels = train_labels.append(test_labels)\n",
    "print(len(pd.unique(train_labels.words)))\n",
    "print(len(pd.unique(test_labels.words)))\n",
    "total_unique_words = len(pd.unique(total_labels.words))\n",
    "total_unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that we have a really big problem => there are 4855 new words that we have never seen. As we saw in our analysis we can't really generalise on never seen words before => this will hinder our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3083\n",
      "2363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3281"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(skype_data_train, label=['phonemes'])\n",
    "test_set, test_labels = get_labels(skype_data_test, label=['phonemes'])\n",
    "\n",
    "train_labels = train_labels.astype('category')\n",
    "test_labels = test_labels.astype('category')\n",
    "\n",
    "total_labels = train_labels.append(test_labels)\n",
    "print(len(pd.unique(train_labels.phonemes)))\n",
    "print(len(pd.unique(test_labels.phonemes)))\n",
    "total_unique_phonemes = len(pd.unique(total_labels.phonemes))\n",
    "total_unique_phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is really promissing. There are only 198 new phonemes, that we haven't seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First model that we will be trying is tree classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_labels = get_labels(skype_data_train, label=[\"words\"])\n",
    "test_set, test_labels = get_labels(skype_data_test, label=[\"words\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"clf\",\n",
    "            DecisionTreeClassifier(criterion=\"entropy\", max_depth=None, splitter=\"best\",\n",
    "                                   min_samples_split=2, random_state=42),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting!\n",
      "Finished!\n",
      "Train accuracy: 0.2925\n",
      "Test accuracy : 0.1195\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "tree_clf_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "print(f\"Train accuracy: {tree_clf_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {tree_clf_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_labels = get_labels(skype_data_train, label=[\"phonemes\"])\n",
    "test_set, test_labels = get_labels(skype_data_test, label=[\"phonemes\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels, label=[\"phonemes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting!\n",
      "Finished!\n",
      "Train accuracy: 0.3282\n",
      "Test accuracy : 0.1399\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "tree_clf_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "print(f\"Train accuracy: {tree_clf_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {tree_clf_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is already quite a promissing success rate for \"just a simple\" tree classifier. Also as we can see, `phonemes` give better results in skype dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look a different kind of classifier => k nearest neighbours. This classifier shouldn't need that much RAM and that much of a computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xkadlec6/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(skype_data_train, label=[\"words\"])\n",
    "test_set, test_labels = get_labels(skype_data_test, label=[\"words\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"scaler\",\n",
    "            StandardScaler()\n",
    "        ),\n",
    "        (\n",
    "            \"clf\",\n",
    "            KNeighborsClassifier(32, weights='distance', n_jobs=-1)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 20, distance => 0.2887, 0.1203\n",
    "# 32, uniform => 0.1700, 0.1343\n",
    "# 32, distance => 0.2912, 0.1216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting!\n",
      "Finished!\n",
      "Train accuracy: 0.2911\n",
      "Test accuracy : 0.1213\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "knn_clf_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "print(f\"Train accuracy: {knn_clf_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {knn_clf_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the search space of 32 nearest neighbours we get around 12% success rate on our test data (which is around 31436 words). I have listed other parameters and their resulting percentages in the comments in the code cell. Also worth noting is that \"StandardScaler\" only worsens (not tested on skype) our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try our luck with phonemes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xkadlec6/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(skype_data_train, label=[\"phonemes\"])\n",
    "test_set, test_labels = get_labels(skype_data_test, label=[\"phonemes\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels, label=[\"phonemes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"clf\",\n",
    "            KNeighborsClassifier(32, weights='distance', n_jobs=-1)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# 20, distance => 0.3221, 0.1377\n",
    "# 32, uniform => 0.2093, 0.1574\n",
    "# 32, distance => 0.3265, 0.1410"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting!\n",
      "Finished!\n",
      "Train accuracy: 0.3265\n",
      "Test accuracy : 0.1410\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "knn_clf_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "print(f\"Train accuracy: {knn_clf_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {knn_clf_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that phonemes are indeed actually better than just words and help us get better predictions. But of course there is also adds the complication of how to make words from these phonemes / make something, that makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xkadlec6/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(skype_data_train, label=[\"phonemes\"])\n",
    "test_set, test_labels = get_labels(skype_data_test, label=[\"phonemes\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels, label=[\"phonemes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"clf\",\n",
    "            RandomForestClassifier(max_depth=12, random_state=42, criterion = 'entropy', n_jobs = -1, min_samples_split = 2)\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Starting!\")\n",
    "rfc_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "print(f\"Train accuracy: {rfc_pipeline.score(train_set, train_labels):.4f}\")\n",
    "print(f\"Test accuracy : {rfc_pipeline.score(test_set, test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAM :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xkadlec6/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(skype_data_train, label=[\"words\"])\n",
    "test_set, test_labels = get_labels(skype_data_test, label=[\"words\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"clf\",\n",
    "            AdaBoostClassifier(random_state=1, n_estimators = 60, learning_rate=0.9)\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0505, 0.0471\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting!\")\n",
    "#abc_pipeline.fit(train_set, train_labels)\n",
    "print(\"Finished!\")\n",
    "\n",
    "#print(f\"Train accuracy: {abc_pipeline.score(train_set, train_labels):.4f}\")\n",
    "#print(f\"Test accuracy : {abc_pipeline.score(test_set, test_labels):.4f}\")\n",
    "\n",
    "print(\"0.0505, 0.0471\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier ended in absolute failure as it wasn't able to get even acceptable results on the train data. And it even took 8 hours to learn (this is because it can only use 1 thread), so this classifier is pretty much worthless to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's now try to bring out the big guns - neural networks. For this I've chosen to use TensorFlow and Keras (PyTorch could also be used). We are able to get reasonably better results but at the cost of long compute times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xkadlec6/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(skype_data_train, label=[\"words\"])\n",
    "test_set, test_labels = get_labels(skype_data_test, label=[\"words\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(707438, 3)\n",
      "(707438,)\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(707438, 20568)\n"
     ]
    }
   ],
   "source": [
    "train_labels = to_categorical(train_labels, num_classes=total_unique_words)\n",
    "test_labels = to_categorical(test_labels, num_classes=total_unique_words)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20568)             5285976   \n",
      "=================================================================\n",
      "Total params: 5,419,352\n",
      "Trainable params: 5,419,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=512, activation='relu', input_dim=3*1))  # first hidden layer\n",
    "model.add(Dense(units=256, activation='relu'))  # second hidden layer\n",
    "# model.add(Dense(units=128, activation='relu'))  # third hidden layer\n",
    "model.add(Dense(units=total_unique_words, activation='softmax'))  # output layer\n",
    "# model.add(Dense(units=128))  # output layer\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "2764/2764 [==============================] - 110s 39ms/step - loss: 7.0326 - accuracy: 0.1377\n",
      "Epoch 2/4\n",
      "2764/2764 [==============================] - 103s 37ms/step - loss: 6.6209 - accuracy: 0.1422\n",
      "Epoch 3/4\n",
      "2764/2764 [==============================] - 102s 37ms/step - loss: 6.5636 - accuracy: 0.1428\n",
      "Epoch 4/4\n",
      "2764/2764 [==============================] - 100s 36ms/step - loss: 6.5314 - accuracy: 0.1433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6d28076080>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_set, train_labels, epochs=4, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8079/8079 [==============================] - 55s 7ms/step - loss: 9.7407 - accuracy: 0.1413\n",
      "test loss, test acc: [9.740732192993164, 0.14129993319511414]\n"
     ]
    }
   ],
   "source": [
    "print(\"test loss, test acc:\", model.evaluate(test_set, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch size = 128:  \n",
    "test loss, test acc: [12.597193717956543, 0.1413850337266922] => 128 epochs  \n",
    "\n",
    "batch size = 256:  \n",
    "test loss, test acc: [9.740732192993164, 0.14129993319511414] => 4 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not used\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "pred_y = model.predict(test_set)\n",
    "print(len(pred_y))\n",
    "print(pred_y[0])\n",
    "\n",
    "pred_y_labels = [0]*len(pred_y)\n",
    "for i in range(len(pred_y)):\n",
    "    pred_y_labels[i] = np.argmax(pred_y[i])\n",
    "    \n",
    "print(pred_y_labels[0])\n",
    "\n",
    "print(classification_report(test_labels, pred_y_labels))\n",
    "\"\"\"\n",
    "print(\"Not used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that changing epoch count doesn't change the results that much and we should try to explore different models / architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xkadlec6/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = get_labels(skype_data_train, label=[\"phonemes\"])\n",
    "test_set, test_labels = get_labels(skype_data_test, label=[\"phonemes\"])\n",
    "\n",
    "train_labels, test_labels, _ = prepare_labels(train_labels, test_labels, label=[\"phonemes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(707438, 3)\n",
      "(707438,)\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(707438, 3281)\n"
     ]
    }
   ],
   "source": [
    "train_labels = to_categorical(train_labels, num_classes=total_unique_phonemes)\n",
    "test_labels = to_categorical(test_labels, num_classes=total_unique_phonemes)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3281)              843217    \n",
      "=================================================================\n",
      "Total params: 976,593\n",
      "Trainable params: 976,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=512, activation='relu', input_dim=3*1))  # first hidden layer\n",
    "model.add(Dense(units=256, activation='relu'))  # second hidden layer\n",
    "# model.add(Dense(units=128, activation='relu'))  # third hidden layer\n",
    "model.add(Dense(units=total_unique_phonemes, activation='softmax'))  # output layer\n",
    "# model.add(Dense(units=128))  # output layer\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "2764/2764 [==============================] - 32s 11ms/step - loss: 4.5476 - accuracy: 0.1544\n",
      "Epoch 2/4\n",
      "2764/2764 [==============================] - 30s 11ms/step - loss: 4.1565 - accuracy: 0.1777\n",
      "Epoch 3/4\n",
      "2764/2764 [==============================] - 30s 11ms/step - loss: 4.1220 - accuracy: 0.1795\n",
      "Epoch 4/4\n",
      "2764/2764 [==============================] - 31s 11ms/step - loss: 4.1061 - accuracy: 0.1802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6d282227b8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_set, train_labels, epochs=4, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8079/8079 [==============================] - 23s 3ms/step - loss: 4.1649 - accuracy: 0.1740\n",
      "test loss, test acc: [4.16491174697876, 0.17395879328250885]\n"
     ]
    }
   ],
   "source": [
    "print(\"test loss, test acc:\", model.evaluate(test_set, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch size = 256:  \n",
    "test loss, test acc: [4.276370048522949, 0.17660073935985565] => 128 epochs  \n",
    "test loss, test acc: [4.16491174697876, 0.17395879328250885] => 4 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that changing epoch count doesn't change the results that much and we should try to explore different models / architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
